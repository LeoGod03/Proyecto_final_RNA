{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb29ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de documentos\n",
    "import pandas as pd\n",
    "def leer_json(archivo):\n",
    "    d = pd.read_json(archivo, lines=True)\n",
    "    return d['text'].to_numpy(), d['klass'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a5e7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7083eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def eliminar_emojis(texto):\n",
    "    # Expresión regular que cubre la mayoría de emojis y pictogramas\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticonos\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # símbolos y pictogramas\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transporte y mapas\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # banderas\n",
    "        \"\\U00002702-\\U000027B0\"  # otros símbolos\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697b506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar más palabras a esta lista si es necesario\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "\n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a minúsculas el texto antes de normalizar\n",
    "    #print(\"antes: \", texto)\n",
    "    texto = texto.lower()\n",
    "    eliminar_emojis(texto)\n",
    "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)  # URLs\n",
    "    texto = re.sub(r\"@\\w+\", \"\", texto)  # Menciones\n",
    "    tokens = word_tokenize(texto)\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "\n",
    "\n",
    "def mi_tokenizador_sin_stopwords(texto):\n",
    "    # Añadimos .lower() para ser consistentes\n",
    "    return [t for t in texto.lower().split() if t not in _STOPWORDS]\n",
    "\n",
    "\n",
    "def mi_tokenizador_con_stemming(texto):\n",
    "\n",
    "    # Esta función ya usaba .lower(), lo cual es correcto\n",
    "    tokens_sin_stopwords = [t for t in texto.lower().split() if t not in _STOPWORDS] \n",
    "    \n",
    "    # Usamos el objeto stemmer_obj que nos pasen como argumento\n",
    "    tokens_stemmed = [stemmer.stem(t) for t in tokens_sin_stopwords]\n",
    "    \n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aba5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorizar_countvectorizer(docs, procesador, tokenizador, rango_ngram):\n",
    "    vec = CountVectorizer(analyzer=\"word\", preprocessor=procesador, tokenizer=tokenizador,  ngram_range=rango_ngram)\n",
    "    X = vec.fit_transform(docs)\n",
    "    return X.toarray(), vec\n",
    "\n",
    "def vectorizar_tfidfvectorizer(docs, procesador, tokenizador, rango_ngram):\n",
    "    vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=procesador, tokenizer= tokenizador, ngram_range= rango_ngram)\n",
    "    X = vec_tfidf.fit_transform(docs)\n",
    "    return X.toarray(), vec_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a05fbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## version cupy\n",
    "import cupy as cp \n",
    "import math\n",
    "\n",
    "# Función de activación sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + cp.exp(-x)) \n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    # return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Establece la semilla para la generación de números aleatorios\n",
    "def seed(random_state=33):\n",
    "    cp.random.seed(random_state)\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    # Calcular el límite de la distribución uniforme\n",
    "    limit = cp.sqrt(6 / (input_size + output_size)) \n",
    "    W = cp.random.uniform(-limit, limit, (input_size, output_size))\n",
    "    return W\n",
    "\n",
    "#Genera la inicialización de con la distribución normal estandar\n",
    "def normal_initialization(input_size, output_size, mean=0.0, std=1.0):\n",
    "    return cp.random.normal(mean, std, (input_size, output_size))\n",
    "\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = cp.random.permutation(n_samples)\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "    \n",
    "    num_batches = math.ceil(n_samples / batch_size)\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(cp.array_split(X_shuffled, num_batches),\n",
    "                                cp.array_split(y_shuffled, num_batches)): \n",
    "        yield X_batch, y_batch\n",
    "\n",
    "    \n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs,fun_init = None, batch_size=32, learning_rate=0.5,tolerancia = 1e-4, patience= 20, min_delta=1e-4,random_state=42):\n",
    "\n",
    "        seed(random_state)\n",
    "        # Definir la tasa de aprendizaje\n",
    "        self.learning_rate = learning_rate\n",
    "        # Definir el número de épocas\n",
    "        self.epochs = epochs\n",
    "        # Definir el tamaño del batch de procesamiento\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "         # definir las capas\n",
    "        self.W1 = fun_init(num_neuronas_ocultas, num_entradas)\n",
    "        self.b1 = cp.zeros((1, num_neuronas_ocultas)) \n",
    "        self.W2 = fun_init(num_salidas, num_neuronas_ocultas)\n",
    "        self.b2 = cp.zeros((1, num_salidas)) \n",
    "\n",
    "        self.tolerancia = tolerancia\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 1. Propagación hacia adelante (Forward pass)\n",
    "        self.X = X\n",
    "        self.z_c1 = X @ self.W1.T + self.b1    \n",
    "        self.a_c1 = sigmoid(self.z_c1)\n",
    "\n",
    "        self.z_c2 = self.a_c1 @ self.W2.T + self.b2\n",
    "        y_pred = sigmoid(self.z_c2)\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        # 2. Cálculo del error con MSE\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * cp.mean((y_pred - y) ** 2) \n",
    "        return error\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        # 3. Propagación hacia atrás (Backward pass)\n",
    "        \n",
    "        # Gradiente de la salida\n",
    "        dE_dy_pred = self.y_pred - self.y\n",
    "        d_y_pred_d_zc2 = sigmoid_derivative(self.y_pred)\n",
    "        delta_c2 = dE_dy_pred * d_y_pred_d_zc2\n",
    "\n",
    "        # Gradiente en la capa oculta\n",
    "        d_zc2_d_a_c1 = delta_c2 @ self.W2\n",
    "        delta_c1 = d_zc2_d_a_c1 * sigmoid_derivative(self.a_c1)\n",
    "\n",
    "        #calcula el gradiente de la función de error respecto a los pesos\n",
    "        self.dE_dW2 = delta_c2.T @ self.a_c1\n",
    "        self.dE_db2 = cp.mean(delta_c2, axis=0, keepdims=True) \n",
    "        self.dE_dW1 = delta_c1.T @ self.X\n",
    "        self.dE_db1 = cp.mean(delta_c1, axis = 0, keepdims=True) \n",
    "\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        # Actualización de pesos de la capa de salida\n",
    "        self.W2 -= self.learning_rate * self.dE_dW2 \n",
    "        self.b2 -= self.learning_rate * self.dE_db2 \n",
    "\n",
    "        # Actuailzación de pesos de la capa oculta\n",
    "        self.W1 -= self.learning_rate * self.dE_dW1 \n",
    "        self.b1 -= self.learning_rate * self.dE_db1 \n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = cp.where(y_pred >= 0.5, 1, 0) \n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        # El entrenamiento ahora asume que X e Y ya son arrays de CuPy\n",
    "        \n",
    "        # --- NUEVO: Inicialización para la paciencia ---\n",
    "        best_error = float('inf')  # El mejor error (más bajo) visto hasta ahora\n",
    "        epochs_no_improve = 0      # Contador de épocas sin mejora\n",
    "        # ------------------------------------------------\n",
    "        \n",
    "        final_epoch_error = 0 \n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_error = 0 \n",
    "            num_batch = 0\n",
    "            \n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                \n",
    "                epoch_error += error.get() # .get() transfiere a CPU\n",
    "                \n",
    "                self.backward()\n",
    "                self.update()\n",
    "                num_batch += 1\n",
    "            \n",
    "            \n",
    "            if num_batch > 0:\n",
    "                avg_epoch_error = epoch_error / num_batch\n",
    "            else:\n",
    "                avg_epoch_error = 0\n",
    "\n",
    "            final_epoch_error = avg_epoch_error \n",
    "\n",
    "            # Imprimir el progreso\n",
    "            # print(f\"Epoch {epoch + 1}/{self.epochs}, Error Promedio: {avg_epoch_error:.8f}\")\n",
    "\n",
    "            # --- 1. Condición de parada (Tolerancia/Umbral) ---\n",
    "            # Tu condición original: si el error es suficientemente bueno.\n",
    "            if avg_epoch_error < self.tolerancia:\n",
    "                print(f\"--- DETENCIÓN TEMPRANA (Tolerancia alcanzada) ---\")\n",
    "                print(f\"Error ({avg_epoch_error:.8f}) < umbral ({self.tolerancia}) en época {epoch + 1}.\")\n",
    "                break \n",
    "\n",
    "            # --- 2. NUEVA Condición de parada (Paciencia / No convergencia) ---\n",
    "            # Comprobar si el error ha mejorado significativamente\n",
    "            \n",
    "            if avg_epoch_error < best_error - self.min_delta:\n",
    "                # ¡Hubo mejora!\n",
    "                best_error = avg_epoch_error  # Actualizamos el mejor error\n",
    "                epochs_no_improve = 0       # Reiniciamos el contador de paciencia\n",
    "            else:\n",
    "                # No hubo mejora significativa\n",
    "                epochs_no_improve += 1\n",
    "            \n",
    "            # Comprobar si nos hemos quedado sin paciencia\n",
    "            if epochs_no_improve >= self.patience:\n",
    "                print(f\"--- DETENCIÓN TEMPRANA (Paciencia agotada) ---\")\n",
    "                print(f\"No hubo mejora significativa en el error durante {self.patience} épocas.\")\n",
    "                print(f\"Último error: {avg_epoch_error:.8f}, Mejor error: {best_error:.8f}\")\n",
    "                break\n",
    "            # -----------------------------------------------------------------\n",
    "\n",
    "        return final_epoch_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3af2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def guardar_resultados(datos, archivo):\n",
    "\n",
    "    df = pd.DataFrame(datos, columns=['klass'])\n",
    "\n",
    "    df['id'] = df.index + 1\n",
    "\n",
    "    df = df[['id', 'klass']]\n",
    "\n",
    "    df.to_csv(archivo, index=False)\n",
    "\n",
    "    print(\"¡Archivo 'output_pandas_Nx1.csv' guardado exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae3b398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Neuronas_ocultas</th>\n",
       "      <th>Init_func</th>\n",
       "      <th>Pesado_terminos</th>\n",
       "      <th>Terminos</th>\n",
       "      <th>Prepocesamiento</th>\n",
       "      <th>Learnin_rate</th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Precision_score</th>\n",
       "      <th>Recall_score</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>1024</td>\n",
       "      <td>normal_initialization</td>\n",
       "      <td>vectorizar_countvectorizer</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>mi_tokenizador_sin_stopwords</td>\n",
       "      <td>0.10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.706713</td>\n",
       "      <td>0.650091</td>\n",
       "      <td>0.640752</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>1024</td>\n",
       "      <td>xavier_initialization</td>\n",
       "      <td>vectorizar_countvectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>mi_tokenizador_con_stemming</td>\n",
       "      <td>0.50</td>\n",
       "      <td>64</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.307479</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>256</td>\n",
       "      <td>normal_initialization</td>\n",
       "      <td>vectorizar_countvectorizer</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>mi_tokenizador_sin_stopwords</td>\n",
       "      <td>0.50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.645181</td>\n",
       "      <td>0.640624</td>\n",
       "      <td>0.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>64</td>\n",
       "      <td>xavier_initialization</td>\n",
       "      <td>vectorizar_countvectorizer</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0.745192</td>\n",
       "      <td>0.733035</td>\n",
       "      <td>0.735428</td>\n",
       "      <td>0.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>512</td>\n",
       "      <td>xavier_initialization</td>\n",
       "      <td>vectorizar_tfidfvectorizer</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>mi_tokenizador_sin_stopwords</td>\n",
       "      <td>0.50</td>\n",
       "      <td>32</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.307479</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>64</td>\n",
       "      <td>xavier_initialization</td>\n",
       "      <td>vectorizar_countvectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>mi_tokenizador_con_stemming</td>\n",
       "      <td>0.50</td>\n",
       "      <td>32</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.307479</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>64</td>\n",
       "      <td>normal_initialization</td>\n",
       "      <td>vectorizar_tfidfvectorizer</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>32</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.357326</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>256</td>\n",
       "      <td>normal_initialization</td>\n",
       "      <td>vectorizar_tfidfvectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.307479</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>256</td>\n",
       "      <td>normal_initialization</td>\n",
       "      <td>vectorizar_tfidfvectorizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.702634</td>\n",
       "      <td>0.697955</td>\n",
       "      <td>0.699233</td>\n",
       "      <td>0.706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>1024</td>\n",
       "      <td>xavier_initialization</td>\n",
       "      <td>vectorizar_countvectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.357326</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Nombre  Neuronas_ocultas              Init_func  \\\n",
       "0    hateval_es              1024  normal_initialization   \n",
       "1    hateval_es              1024  xavier_initialization   \n",
       "2    hateval_es               256  normal_initialization   \n",
       "3    hateval_es                64  xavier_initialization   \n",
       "4    hateval_es               512  xavier_initialization   \n",
       "..          ...               ...                    ...   \n",
       "295  hateval_es                64  xavier_initialization   \n",
       "296  hateval_es                64  normal_initialization   \n",
       "297  hateval_es               256  normal_initialization   \n",
       "298  hateval_es               256  normal_initialization   \n",
       "299  hateval_es              1024  xavier_initialization   \n",
       "\n",
       "                Pesado_terminos Terminos               Prepocesamiento  \\\n",
       "0    vectorizar_countvectorizer   (2, 2)  mi_tokenizador_sin_stopwords   \n",
       "1    vectorizar_countvectorizer   (1, 2)   mi_tokenizador_con_stemming   \n",
       "2    vectorizar_countvectorizer   (2, 2)  mi_tokenizador_sin_stopwords   \n",
       "3    vectorizar_countvectorizer   (2, 2)                           NaN   \n",
       "4    vectorizar_tfidfvectorizer   (2, 2)  mi_tokenizador_sin_stopwords   \n",
       "..                          ...      ...                           ...   \n",
       "295  vectorizar_countvectorizer   (1, 2)   mi_tokenizador_con_stemming   \n",
       "296  vectorizar_tfidfvectorizer   (2, 2)                           NaN   \n",
       "297  vectorizar_tfidfvectorizer   (1, 2)                           NaN   \n",
       "298  vectorizar_tfidfvectorizer   (1, 1)                           NaN   \n",
       "299  vectorizar_countvectorizer   (1, 2)                           NaN   \n",
       "\n",
       "     Learnin_rate  Batch_size  Precision_score  Recall_score  F1_score  \\\n",
       "0            0.10          16         0.706713      0.650091  0.640752   \n",
       "1            0.50          64         0.222000      0.500000  0.307479   \n",
       "2            0.50          16         0.677778      0.645181  0.640624   \n",
       "3            0.01          64         0.745192      0.733035  0.735428   \n",
       "4            0.50          32         0.222000      0.500000  0.307479   \n",
       "..            ...         ...              ...           ...       ...   \n",
       "295          0.50          32         0.222000      0.500000  0.307479   \n",
       "296          0.50          32         0.278000      0.500000  0.357326   \n",
       "297          0.50          16         0.222000      0.500000  0.307479   \n",
       "298          0.10          32         0.702634      0.697955  0.699233   \n",
       "299          0.50          16         0.278000      0.500000  0.357326   \n",
       "\n",
       "     Accuracy  \n",
       "0       0.678  \n",
       "1       0.444  \n",
       "2       0.668  \n",
       "3       0.744  \n",
       "4       0.444  \n",
       "..        ...  \n",
       "295     0.444  \n",
       "296     0.556  \n",
       "297     0.444  \n",
       "298     0.706  \n",
       "299     0.556  \n",
       "\n",
       "[300 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hateval_es = pd.read_csv('Resultados_temporales_pract2_hateval_es.csv')\n",
    "df_hateval_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9224c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Neuronas_ocultas</th>\n",
       "      <th>Init_func</th>\n",
       "      <th>Pesado_terminos</th>\n",
       "      <th>Terminos</th>\n",
       "      <th>Prepocesamiento</th>\n",
       "      <th>Learnin_rate</th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Precision_score</th>\n",
       "      <th>Recall_score</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>512</td>\n",
       "      <td>xavier_initialization</td>\n",
       "      <td>vectorizar_tfidfvectorizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>mi_tokenizador_sin_stopwords</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0.782520</td>\n",
       "      <td>0.518018</td>\n",
       "      <td>0.395822</td>\n",
       "      <td>0.572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>64</td>\n",
       "      <td>xavier_initialization</td>\n",
       "      <td>vectorizar_countvectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.779193</td>\n",
       "      <td>0.769055</td>\n",
       "      <td>0.771709</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>64</td>\n",
       "      <td>xavier_initialization</td>\n",
       "      <td>vectorizar_countvectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.776819</td>\n",
       "      <td>0.767256</td>\n",
       "      <td>0.769809</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>512</td>\n",
       "      <td>xavier_initialization</td>\n",
       "      <td>vectorizar_countvectorizer</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>mi_tokenizador_sin_stopwords</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0.775466</td>\n",
       "      <td>0.777675</td>\n",
       "      <td>0.776187</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>hateval_es</td>\n",
       "      <td>128</td>\n",
       "      <td>xavier_initialization</td>\n",
       "      <td>vectorizar_countvectorizer</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>mi_tokenizador_con_stemming</td>\n",
       "      <td>0.10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.775407</td>\n",
       "      <td>0.774046</td>\n",
       "      <td>0.774646</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Nombre  Neuronas_ocultas              Init_func  \\\n",
       "100  hateval_es               512  xavier_initialization   \n",
       "143  hateval_es                64  xavier_initialization   \n",
       "237  hateval_es                64  xavier_initialization   \n",
       "46   hateval_es               512  xavier_initialization   \n",
       "136  hateval_es               128  xavier_initialization   \n",
       "\n",
       "                Pesado_terminos Terminos               Prepocesamiento  \\\n",
       "100  vectorizar_tfidfvectorizer   (1, 1)  mi_tokenizador_sin_stopwords   \n",
       "143  vectorizar_countvectorizer   (1, 2)                           NaN   \n",
       "237  vectorizar_countvectorizer   (1, 2)                           NaN   \n",
       "46   vectorizar_countvectorizer   (1, 1)  mi_tokenizador_sin_stopwords   \n",
       "136  vectorizar_countvectorizer   (1, 2)   mi_tokenizador_con_stemming   \n",
       "\n",
       "     Learnin_rate  Batch_size  Precision_score  Recall_score  F1_score  \\\n",
       "100          0.01          64         0.782520      0.518018  0.395822   \n",
       "143          0.10          16         0.779193      0.769055  0.771709   \n",
       "237          0.10          32         0.776819      0.767256  0.769809   \n",
       "46           0.01          64         0.775466      0.777675  0.776187   \n",
       "136          0.10          16         0.775407      0.774046  0.774646   \n",
       "\n",
       "     Accuracy  \n",
       "100     0.572  \n",
       "143     0.778  \n",
       "237     0.776  \n",
       "46      0.778  \n",
       "136     0.778  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hateval_es.sort_values(by='Precision_score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02854ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\gpu_env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Y_train.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m     62\u001b[39m     Y_train = Y_train.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m error = \u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vectorizado\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperimentos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - último error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m y_predicha = mlp.predict(X_test_vectorizado)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mMLP_TODO.train\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    131\u001b[39m y_pred = \u001b[38;5;28mself\u001b[39m.forward(X_batch)\n\u001b[32m    132\u001b[39m error = \u001b[38;5;28mself\u001b[39m.loss_function_MSE(y_pred, y_batch)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m epoch_error += \u001b[43merror\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# .get() transfiere a CPU\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mself\u001b[39m.backward()\n\u001b[32m    137\u001b[39m \u001b[38;5;28mself\u001b[39m.update()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "le = LabelEncoder()\n",
    "\n",
    "X, Y =  leer_json('../Datasets/dataset_humor_train.json')\n",
    "Y_encoded = le.fit_transform(Y)\n",
    "\n",
    "#tamaño de los k-folds\n",
    "skf = StratifiedKFold(n_splits=5)  \n",
    "for k, (index_train, index_test) in enumerate(skf.split(X, Y_encoded), start=1):\n",
    "        \n",
    "    # index_train y index_test obtienen los índices de las muestras para procesar\n",
    "    X_train_cv, X_test_cv = X[index_train], X[index_test]\n",
    "\n",
    "    Y_train_cv, Y_test_cv = Y_encoded[index_train], Y_encoded[index_test]\n",
    "        \n",
    "    df_mejores = df_hateval_es.sort_values(by='F1_score', ascending=False).head(5)\n",
    "\n",
    "    experimentos = 1\n",
    "            \n",
    "    for _, filas in df_mejores.iterrows():\n",
    "        neuronas_ocultas = filas['Neuronas_ocultas']\n",
    "        func_init = filas['Init_func']\n",
    "        pes_term = filas['Pesado_terminos']\n",
    "        terminos = filas['Terminos']\n",
    "        prepo = filas['Prepocesamiento']\n",
    "        lr = filas['Learnin_rate']\n",
    "        batch_size = filas['Batch_size']\n",
    "        salidas = 1\n",
    "        epocas = 101\n",
    "        terminos = ast.literal_eval(terminos)\n",
    "        semilla = 42\n",
    "\n",
    "        init_func = xavier_initialization if func_init == 'xavier_initialization' else normal_initialization\n",
    "        vectorizador = vectorizar_countvectorizer if pes_term == 'vectorizar_countvectorizer' else vectorizar_tfidfvectorizer\n",
    "        prepocesamiento = None\n",
    "        if prepo == 'mi_tokenizador_sin_stopwords':\n",
    "            prepocesamiento = mi_tokenizador_sin_stopwords \n",
    "        if prepo == 'mi_tokenizador_con_stemming':\n",
    "            prepocesamiento = mi_tokenizador_con_stemming\n",
    "                \n",
    "                \n",
    "        try:\n",
    "            X_train_vectorizado, vec_train = vectorizador(X_train_cv, normaliza_texto, prepocesamiento, terminos)\n",
    "            X_test_vectorizado = vec_train.transform(X_test_cv).toarray()\n",
    "             #print(X_train_vectorizado)\n",
    "            entradas = X_train_vectorizado.shape[1]\n",
    "\n",
    "            X_train_vectorizado = X_train_vectorizado.astype('float16') \n",
    "            X_test_vectorizado = X_test_vectorizado.astype('float16')\n",
    "\n",
    "            mlp = MLP_TODO(entradas, neuronas_ocultas, salidas, epocas, fun_init= init_func, batch_size=min(batch_size, X_train_vectorizado.shape[0]), learning_rate=lr, random_state=semilla)\n",
    "\n",
    "            X_train_vectorizado = cp.asarray(X_train_vectorizado)\n",
    "            X_test_vectorizado = cp.asarray(X_test_vectorizado)\n",
    "            Y_train = cp.asanyarray(Y_train_cv)\n",
    "            if Y_train.ndim == 1:\n",
    "                Y_train = Y_train.reshape(-1, 1)\n",
    "\n",
    "            error = mlp.train(X_train_vectorizado, Y_train)\n",
    "            print(f\"{experimentos} - último error: {error}\")\n",
    "\n",
    "            y_predicha = mlp.predict(X_test_vectorizado)\n",
    "            y_predicha = y_predicha.get()\n",
    "                \n",
    "            print(f'{experimentos} - P_score: {precision_score(Y_test_cv, y_predicha)}')\n",
    "            print(f'{experimentos} - F_score: {f1_score(Y_test_cv, y_predicha)}')\n",
    "            print(f'{experimentos} - Recall_score: {recall_score(Y_test_cv, y_predicha)}')\n",
    "            print(f'{experimentos} - Accuracy: {accuracy_score(Y_test_cv, y_predicha)}')\n",
    "            print(neuronas_ocultas, func_init, pes_term, terminos,prepo,lr, batch_size)\n",
    "\n",
    "            experimentos += 1\n",
    "\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Fallo la configuración: {e}\")\n",
    "            print(neuronas_ocultas, func_init, pes_term, terminos,prepo,lr, batch_size)\n",
    "                \n",
    "                #print(neuronas_ocultas, func_init, pes_term, terminos,prepo,lr, batch_size)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e932367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 xavier_initialization vectorizar_countvectorizer (1, 1) mi_tokenizador_sin_stopwords 0.01 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\gpu_env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - último error: 0.003730449677598128\n",
      "¡Archivo 'output_pandas_Nx1.csv' guardado exitosamente!\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "sum:1707\n",
      "512 xavier_initialization vectorizar_countvectorizer (1, 1) mi_tokenizador_sin_stopwords 0.01 64\n",
      "128 xavier_initialization vectorizar_countvectorizer (1, 2) mi_tokenizador_sin_stopwords 0.01 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\gpu_env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\gpu_env\\Lib\\site-packages\\cupy\\_creation\\from_data.py:88: PerformanceWarning: Using synchronous transfer as pinned memory (1694430400 bytes) could not be allocated. This generally occurs because of insufficient host memory. The original error was: cudaErrorMemoryAllocation: out of memory\n",
      "  return _core.array(a, dtype, False, order, blocking=blocking)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallo la configuración: Out of memory allocating 1,694,430,720 bytes (allocated so far: 10,239,311,360 bytes).\n",
      "128 xavier_initialization vectorizar_countvectorizer (1, 2) mi_tokenizador_sin_stopwords 0.01 32\n",
      "64 xavier_initialization vectorizar_countvectorizer (1, 2) mi_tokenizador_sin_stopwords 0.01 64\n",
      "--- DETENCIÓN TEMPRANA (Paciencia agotada) ---\n",
      "No hubo mejora significativa en el error durante 20 épocas.\n",
      "Último error: 0.00086490, Mejor error: 0.00095382\n",
      "2 - último error: 0.0008648976714958546\n",
      "¡Archivo 'output_pandas_Nx1.csv' guardado exitosamente!\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "sum:1522\n",
      "64 xavier_initialization vectorizar_countvectorizer (1, 2) mi_tokenizador_sin_stopwords 0.01 64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import ast\n",
    "import pandas as pd\n",
    "le = LabelEncoder()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, Y =  leer_json('../Datasets/dataset_humor_train.json')\n",
    "#Y_encoded = le.fit_transform(Y)\n",
    "X_test, _ = leer_json('../Datasets/dataset_humor_test.json')\n",
    "#X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify= Y_encoded, random_state=42)\n",
    "df_mejores = df_hateval_es.sort_values(by='F1_score', ascending=False).head(3)\n",
    "\n",
    "\n",
    "experimentos = 1\n",
    "            \n",
    "for _, filas in df_mejores.iterrows():\n",
    "    neuronas_ocultas = filas['Neuronas_ocultas']\n",
    "    func_init = filas['Init_func']\n",
    "    pes_term = filas['Pesado_terminos']\n",
    "    terminos = filas['Terminos']\n",
    "    prepo = filas['Prepocesamiento']\n",
    "    lr = filas['Learnin_rate']\n",
    "    batch_size = filas['Batch_size']\n",
    "    salidas = 1\n",
    "    epocas = 256\n",
    "    terminos = ast.literal_eval(terminos)\n",
    "    semilla = 42\n",
    "\n",
    "    init_func = xavier_initialization if func_init == 'xavier_initialization' else normal_initialization\n",
    "    vectorizador = vectorizar_countvectorizer if pes_term == 'vectorizar_countvectorizer' else vectorizar_tfidfvectorizer\n",
    "    prepocesamiento = None\n",
    "    if prepo == 'mi_tokenizador_sin_stopwords':\n",
    "        prepocesamiento = mi_tokenizador_sin_stopwords \n",
    "    if prepo == 'mi_tokenizador_con_stemming':\n",
    "        prepocesamiento = mi_tokenizador_con_stemming\n",
    "                \n",
    "                \n",
    "    try:\n",
    "        print(neuronas_ocultas, func_init, pes_term, terminos,prepo,lr, batch_size)\n",
    "        X_train_vectorizado, vec_train = vectorizador(X, normaliza_texto, prepocesamiento, terminos)\n",
    "        X_test_vectorizado = vec_train.transform(X_test).toarray()\n",
    "\n",
    "            #print(X_train_vectorizado)\n",
    "        entradas = X_train_vectorizado.shape[1]\n",
    "\n",
    "        X_train_vectorizado = X_train_vectorizado.astype('float16') \n",
    "        X_test_vectorizado = X_test_vectorizado.astype('float16')\n",
    "\n",
    "        mlp = MLP_TODO(entradas, neuronas_ocultas, salidas, epocas, fun_init= init_func, batch_size=min(batch_size, X_train_vectorizado.shape[0]), learning_rate=lr, random_state=semilla)\n",
    "\n",
    "        X_train_vectorizado = cp.asarray(X_train_vectorizado)\n",
    "        X_test_vectorizado = cp.asarray(X_test_vectorizado)\n",
    "        Y_train = cp.asanyarray(Y)\n",
    "        if Y_train.ndim == 1:\n",
    "            Y_train = Y_train.reshape(-1, 1)\n",
    "        \n",
    "        nombre_archivo =f'{neuronas_ocultas}_{func_init}_{pes_term}_{terminos}_{prepo}_{lr}_{batch_size}.csv'\n",
    "\n",
    "        error = mlp.train(X_train_vectorizado, Y_train)\n",
    "        print(f\"{experimentos} - último error: {error}\")\n",
    "\n",
    "        y_predicha = mlp.predict(X_test_vectorizado)\n",
    "        y_predicha = y_predicha.get()\n",
    "        guardar_resultados(y_predicha,f'../Resultados_Leo/{nombre_archivo}')\n",
    "        print(y_predicha)\n",
    "        print(f'sum:{y_predicha.sum()}')        \n",
    "        print(neuronas_ocultas, func_init, pes_term, terminos,prepo,lr, batch_size)\n",
    "\n",
    "\n",
    "        experimentos += 1\n",
    "\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Fallo la configuración: {e}\")\n",
    "        print(neuronas_ocultas, func_init, pes_term, terminos,prepo,lr, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
