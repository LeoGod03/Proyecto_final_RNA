{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a62f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- A. FOCAL LOSS (Mejor que CrossEntropy con pesos simples) ---\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss) # probabilidad de la clase correcta\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        else:\n",
    "            return focal_loss.sum()\n",
    "\n",
    "# --- B. NUEVA ARQUITECTURA (Embudo + GELU + Menor Dropout) ---\n",
    "class ImprovedMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ImprovedMLP, self).__init__()\n",
    "        \n",
    "        # Arquitectura de \"Embudo\" para forzar compresión de características\n",
    "        # 900 -> 512 -> 256 -> 128 -> 2\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.drop1 = nn.Dropout(0.3) # Menos agresivo que 0.7\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.output = nn.Linear(128, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Usamos GELU en lugar de ReLU (funciona mejor en NLP/Embeddings)\n",
    "        x = self.drop1(F.gelu(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(F.gelu(self.bn2(self.fc2(x))))\n",
    "        x = self.drop3(F.gelu(self.bn3(self.fc3(x))))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708ba7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset...\n",
      "Procesando embeddings...\n",
      "Dimensión total de entrada: (10400, 900)\n",
      "Datos listos -> Train: (8840, 900), Val: (1560, 900)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 0. CARGA Y PREPARACIÓN DE DATOS ---\n",
    "\n",
    "print(\"Cargando dataset...\")\n",
    "# Asegúrate de que la ruta sea correcta según tu estructura de carpetas\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_train_embeddings.json\", lines=True)\n",
    "\n",
    "print(\"Procesando embeddings...\")\n",
    "# Extraer y apilar los arrays de numpy de las columnas\n",
    "we_ft = np.stack(dataset['we_ft'].values) # FastText General\n",
    "we_mx = np.stack(dataset['we_mx'].values) # FastText México\n",
    "we_es = np.stack(dataset['we_es'].values) # FastText España\n",
    "\n",
    "# Concatenar los 3 vectores (N, 900)\n",
    "X_full = np.concatenate([we_ft, we_mx, we_es], axis=1)\n",
    "Y_full = dataset['klass'].to_numpy()\n",
    "\n",
    "print(f\"Dimensión total de entrada: {X_full.shape}\")\n",
    "\n",
    "# Dividir en Train y Validation\n",
    "# NOTA: Mantenemos random_state=42 fijo aquí para que todos los modelos del ensamble\n",
    "# se entrenen y validen con los mismos datos. La \"variedad\" vendrá de la inicialización de la red.\n",
    "X_tr, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_full, Y_full, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=Y_full # Importante para mantener la proporción de humor\n",
    ")\n",
    "\n",
    "print(f\"Datos listos -> Train: {X_tr.shape}, Val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca60844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Función para generar mixup (pon esto fuera de la clase o función)\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# 2. Nueva función de pérdida para mixup\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d6c29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedFusionMLP(nn.Module):\n",
    "    def __init__(self, output_size=2):\n",
    "        super(GatedFusionMLP, self).__init__()\n",
    "        \n",
    "        # Dimensiones originales de cada vector\n",
    "        self.dim = 300 \n",
    "        \n",
    "        # --- Mecanismo de Atención ---\n",
    "        # Calcula un \"score\" de importancia para cada fuente (FT, MX, ES)\n",
    "        self.attention_net = nn.Sequential(\n",
    "            nn.Linear(900, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 3), # 3 pesos de salida (uno para cada vector)\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # --- Clasificador Principal (ahora recibe 300, no 900) ---\n",
    "        # Porque haremos una suma ponderada de los vectores\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(300, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x viene con shape (Batch, 900)\n",
    "        # Separamos los 3 embeddings originales\n",
    "        ft = x[:, 0:300]\n",
    "        mx = x[:, 300:600]\n",
    "        es = x[:, 600:900]\n",
    "        \n",
    "        # 1. Calcular pesos de atención\n",
    "        weights = self.attention_net(x) # (Batch, 3)\n",
    "        w_ft = weights[:, 0].unsqueeze(1)\n",
    "        w_mx = weights[:, 1].unsqueeze(1)\n",
    "        w_es = weights[:, 2].unsqueeze(1)\n",
    "        \n",
    "        # 2. Fusión Ponderada (Weighted Average)\n",
    "        # El vector resultante es una mezcla inteligente de los 3\n",
    "        fused_vector = (ft * w_ft) + (mx * w_mx) + (es * w_es) # (Batch, 300)\n",
    "        \n",
    "        # 3. Clasificación\n",
    "        out = self.classifier(fused_vector)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "621e02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo_semilla(seed, X_tr, Y_train, X_val, Y_val, config):\n",
    "    # 1. Reproducibilidad para esta iteración\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Datos a tensores\n",
    "    X_t = torch.from_numpy(X_tr).float().to(device)\n",
    "    Y_t = torch.from_numpy(Y_train).long().to(device)\n",
    "    X_v = torch.from_numpy(X_val).float().to(device)\n",
    "    Y_v = torch.from_numpy(Y_val).long().to(device) # No necesitamos pesos aquí para la métrica, solo para el Loss\n",
    "    \n",
    "    # DataLoader\n",
    "    dataset = TensorDataset(X_t, Y_t)\n",
    "    loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Modelo\n",
    "    model = ImprovedMLP(input_size=X_tr.shape[1], output_size=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizador y Loss\n",
    "    # Usamos Focal Loss en lugar de pesos manuales (alpha maneja el balance)\n",
    "    criterion = FocalLoss(alpha=0.75, gamma=2.0) \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['wd'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=False)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"--- Iniciando Semilla {seed} ---\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # APLICAR MIXUP\n",
    "            # Solo aplicamos mixup con cierta probabilidad (o siempre, prueba ambos)\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(xb, yb, alpha=0.4) # alpha 0.2 a 0.4 suele ir bien\n",
    "            \n",
    "            logits = model(mixed_x)\n",
    "            \n",
    "            # Usamos el criterio mixup\n",
    "            loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_val = model(X_v)\n",
    "            # Calculamos Loss para monitoreo\n",
    "            val_loss = criterion(logits_val, Y_v).item()\n",
    "            \n",
    "            preds = torch.argmax(logits_val, dim=1).cpu().numpy()\n",
    "            f1 = f1_score(Y_val, preds, average='macro')\n",
    "            \n",
    "        # Scheduler basado en F1 (queremos maximizar)\n",
    "        scheduler.step(f1)\n",
    "        \n",
    "        # Guardado del mejor estado en RAM\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= config['patience']:\n",
    "            break\n",
    "            \n",
    "    print(f\"Semilla {seed} terminada. Mejor Val F1 Macro: {best_f1:.4f}\")\n",
    "    \n",
    "    # Guardar modelo en disco para el ensamble\n",
    "    torch.save(best_model_state, f\"modelo_ensemble_seed_{seed}.pth\")\n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ad40066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo_semilla(seed, X_tr, Y_train, X_val, Y_val, config):\n",
    "    # 1. Reproducibilidad\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Datos a tensores\n",
    "    X_t = torch.from_numpy(X_tr).float().to(device)\n",
    "    Y_t = torch.from_numpy(Y_train).long().to(device)\n",
    "    X_v = torch.from_numpy(X_val).float().to(device)\n",
    "    Y_v = torch.from_numpy(Y_val).long().to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X_t, Y_t)\n",
    "    loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    \n",
    "    # --- CAMBIO IMPORTANTE AQUÍ ---\n",
    "    # Instanciamos la red de Fusión (GatedFusionMLP) en lugar del MLP simple.\n",
    "    # Esta red ya sabe que la entrada es de 900 (300+300+300) internamente.\n",
    "    model = GatedFusionMLP(output_size=2) \n",
    "    model.to(device)\n",
    "    \n",
    "    # Loss y Optimizador\n",
    "    criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['wd'])\n",
    "    \n",
    "    # Scheduler: Reduce el LR si el F1 no mejora\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"--- Iniciando Semilla {seed} con GatedFusionMLP ---\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        \n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # NOTA: Si quisieras combinar Táctica 1 (Mixup) con Táctica 2,\n",
    "            # aquí iría el bloque de mixup_data y mixup_criterion.\n",
    "            # Por ahora, lo dejamos en entrenamiento estándar:\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_val = model(X_v)\n",
    "            preds = torch.argmax(logits_val, dim=1).cpu().numpy()\n",
    "            f1 = f1_score(Y_val, preds, average='macro')\n",
    "            \n",
    "        scheduler.step(f1)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= config['patience']:\n",
    "            break\n",
    "            \n",
    "    print(f\"Semilla {seed} terminada. Mejor Val F1 Macro: {best_f1:.4f}\")\n",
    "    \n",
    "    # Guardamos el modelo\n",
    "    torch.save(best_model_state, f\"modelo_ensemble_seed_{seed}.pth\")\n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c74c7096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_semillas_fibonacci(cantidad, start_index=15):\n",
    "    \"\"\"\n",
    "    Genera una lista de semillas basada en la secuencia de Fibonacci.\n",
    "    \n",
    "    Args:\n",
    "        cantidad (int): Cuántas semillas necesitas (ej: 5 para tu ensamble).\n",
    "        start_index (int): Cuántos pasos saltar al inicio para evitar números \n",
    "                           pequeños como 0, 1, 1, 2. Por defecto 15 empieza en 610.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de enteros listos para usar como semillas.\n",
    "    \"\"\"\n",
    "    semillas = []\n",
    "    a, b = 0, 1\n",
    "    \n",
    "    # 1. Avanzar la secuencia hasta el punto de inicio (offset)\n",
    "    for _ in range(start_index):\n",
    "        a, b = b, a + b\n",
    "        \n",
    "    # 2. Generar las semillas solicitadas\n",
    "    for _ in range(cantidad):\n",
    "        # Aplicamos modulo 2^32 - 1 para asegurar compatibilidad con numpy/torch\n",
    "        # aunque Python soporta enteros infinitos, las librerias de ML a veces no.\n",
    "        seed_val = a % (2**32 - 1)\n",
    "        semillas.append(seed_val)\n",
    "        \n",
    "        # Siguiente paso de Fibonacci\n",
    "        a, b = b, a + b\n",
    "        \n",
    "    return semillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18edae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Semilla 21 con GatedFusionMLP ---\n",
      "Semilla 21 terminada. Mejor Val F1 Macro: 0.8009\n",
      "--- Iniciando Semilla 34 con GatedFusionMLP ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m f1_scores = []\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     score = \u001b[43mentrenar_modelo_semilla\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     f1_scores.append(score)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPromedio F1 Macro (Validación) del Ensamble individual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(f1_scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mentrenar_modelo_semilla\u001b[39m\u001b[34m(seed, X_tr, Y_train, X_val, Y_val, config)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m     39\u001b[39m     model.train()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\gpu_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:697\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    696\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[32m    700\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\gpu_env\\Lib\\site-packages\\torch\\autograd\\profiler.py:738\u001b[39m, in \u001b[36mrecord_function.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m.record = torch.ops.profiler._record_function_enter_new(\n\u001b[32m    734\u001b[39m         \u001b[38;5;28mself\u001b[39m.name, \u001b[38;5;28mself\u001b[39m.args\n\u001b[32m    735\u001b[39m     )\n\u001b[32m    736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_callbacks_on_exit:\n\u001b[32m    740\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# --- CONFIGURACIÓN GLOBAL ---\n",
    "config = {\n",
    "    'batch_size': 32,      # Bajamos un poco para regularizar\n",
    "    'lr': 0.001,           # AdamW suele preferir LRs más bajos que 0.1\n",
    "    'wd': 0.05,            # Weight Decay más fuerte para evitar overfitting\n",
    "    'epochs': 100,\n",
    "    'patience': 15\n",
    "}\n",
    "\n",
    "# Lista de semillas para el ensamble (puedes usar 3, 5 o 7)\n",
    "seeds = generar_semillas_fibonacci(cantidad=5, start_index=8)\n",
    "\n",
    "# 1. ENTRENAMIENTO DEL ENSAMBLE\n",
    "f1_scores = []\n",
    "for seed in seeds:\n",
    "    score = entrenar_modelo_semilla(seed, X_tr, Y_train, X_val, Y_val, config)\n",
    "    f1_scores.append(score)\n",
    "\n",
    "print(f\"\\nPromedio F1 Macro (Validación) del Ensamble individual: {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "# 2. INFERENCIA CON ENSAMBLE ACTUALIZADA (PARA TÁCTICA 2)\n",
    "def predecir_ensamble(seeds, archivo_test_path):\n",
    "    print(\"\\n--- Iniciando Inferencia de Ensamble (Gated Fusion) ---\")\n",
    "    \n",
    "    # Cargar datos de test\n",
    "    df_test = pd.read_json(archivo_test_path, lines=True)\n",
    "    we_ft_t = np.stack(df_test['we_ft'].values)\n",
    "    we_mx_t = np.stack(df_test['we_mx'].values)\n",
    "    we_es_t = np.stack(df_test['we_es'].values)\n",
    "    \n",
    "    # Concatenamos igual que en el train (N, 900)\n",
    "    # La red GatedFusionMLP se encarga de separarlos internamente\n",
    "    X_t_np = np.concatenate([we_ft_t, we_mx_t, we_es_t], axis=1)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_test_tensor = torch.from_numpy(X_t_np).float().to(device)\n",
    "    \n",
    "    # Acumulador de probabilidades\n",
    "    probs_totales = np.zeros((len(X_test_tensor), 2))\n",
    "    \n",
    "    # --- CAMBIO AQUÍ: Instanciar el modelo correcto ---\n",
    "    model = GatedFusionMLP(output_size=2).to(device)\n",
    "    \n",
    "    for seed in seeds:\n",
    "        nombre_archivo = f\"modelo_ensemble_seed_{seed}.pth\"\n",
    "        # Cargar pesos\n",
    "        model.load_state_dict(torch.load(nombre_archivo))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(X_test_tensor)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            probs_totales += probs\n",
    "            \n",
    "    # Promediar y sacar argmax\n",
    "    probs_promedio = probs_totales / len(seeds)\n",
    "    y_pred_final = np.argmax(probs_promedio, axis=1)\n",
    "    \n",
    "    return y_pred_final\n",
    "\n",
    "# 3. GENERAR ARCHIVO\n",
    "y_pred_test = predecir_ensamble(seeds, \"../Datasets/dataset_humor_test_embeddings.json\")\n",
    "\n",
    "os.makedirs('resultados_focal_loss', exist_ok=True)\n",
    "\n",
    "# Guardar\n",
    "nombre_salida = \"resultados_focal_loss/submission_ensemble_focal_loss_v5.csv\"\n",
    "df_out = pd.DataFrame({'klass': y_pred_test})\n",
    "df_out['id'] = df_out.index + 1\n",
    "df_out = df_out[['id', 'klass']]\n",
    "df_out.to_csv(nombre_salida, index=False)\n",
    "\n",
    "print(f\"¡Archivo generado exitosamente: {nombre_salida}!\")\n",
    "print(\"Distribución de predicciones:\", np.unique(y_pred_test, return_counts=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
