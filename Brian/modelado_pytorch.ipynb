{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5c0b8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de entrenamiento\n",
      "klass\n",
      "0    6588\n",
      "1    3812\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_train.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X_train = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y_train = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "babc0687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de prueba\n",
      "klass\n",
      "-1    5600\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_test.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de prueba\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X_test = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y_test = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "028c4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide el conjunto de entrenamiento en:  entrenamiento (90%) y validación (10%)\n",
    "X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=0.1, stratify=Y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4ce8ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def eliminar_emojis(texto):\n",
    "    # Expresión regular que cubre la mayoría de emojis y pictogramas\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticonos\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # símbolos y pictogramas\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transporte y mapas\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # banderas\n",
    "        \"\\U00002702-\\U000027B0\"  # otros símbolos\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ab414fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"english\")  # agregar más palabras a esta lista si es necesario\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "def mi_preprocesamiento_factory(modo):\n",
    "    \"\"\"\n",
    "    Devuelve una función de preprocesamiento y tokenización según el modo:\n",
    "    - 'normalizacion'\n",
    "    - 'normalizacion_stopwords'\n",
    "    - 'normalizacion_stopwords_stem'\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocesamiento(texto):\n",
    "        texto = texto.lower()\n",
    "        #texto = eliminar_emojis(texto)\n",
    "        texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)\n",
    "        texto = re.sub(r\"@\\w+\", \"\", texto)\n",
    "        texto = normaliza_texto(texto, punct=True)\n",
    "        return texto\n",
    "\n",
    "    def tokenizador(texto):\n",
    "        tokens = word_tokenize(texto)\n",
    "        if modo in ['normalizacion_stopwords', 'normalizacion_stopwords_stem']:\n",
    "            tokens = [t for t in tokens if t not in _STOPWORDS and len(t) > 2]\n",
    "        if modo == 'normalizacion_stopwords_stem':\n",
    "            tokens = [stemmer.stem(t) for t in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return preprocesamiento, tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17f6a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class Vec_TFID:\n",
    "    def __init__(self, modo_preproc='normalizacion'):\n",
    "        self.vec_tfidf = None\n",
    "        self.modo_preproc = modo_preproc\n",
    "\n",
    "    def create_matriz_TFID(self, X_train, ngram_config, max_config):\n",
    "        preproc, token = mi_preprocesamiento_factory(self.modo_preproc)\n",
    "        self.vec_tfidf = TfidfVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            preprocessor=preproc,\n",
    "            tokenizer=token,\n",
    "            ngram_range=ngram_config,\n",
    "            max_features=max_config\n",
    "        )\n",
    "        X_tfidf = self.vec_tfidf.fit_transform(X_train)\n",
    "        return X_tfidf.toarray()\n",
    "\n",
    "    def tranform_matriz_TFID(self, X_test):\n",
    "        X_tfid = self.vec_tfidf.transform(X_test)\n",
    "        return X_tfid.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05546e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "class Vec_Count:\n",
    "    def __init__(self, modo_preproc='normalizacion'):\n",
    "        self.vec = None\n",
    "        self.modo_preproc = modo_preproc\n",
    "\n",
    "    def create_matriz_Count(self, X_train, ngram_config, max_config):\n",
    "        preproc, token = mi_preprocesamiento_factory(self.modo_preproc)\n",
    "        self.vec = CountVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            preprocessor=preproc,\n",
    "            tokenizer=token,\n",
    "            ngram_range=ngram_config,\n",
    "            max_features=max_config\n",
    "        )\n",
    "        X_vec = self.vec.fit_transform(X_train)\n",
    "        return X_vec.toarray().astype(np.float32)\n",
    "\n",
    "    def transform_matriz_Count(self, X_test):\n",
    "        X_vec = self.vec.transform(X_test)\n",
    "        return X_vec.toarray().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ba8fe184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "USE_TFIDF = True      # True: TF-IDF, False: Count\n",
    "NGRAM_RANGE = (1, 2)\n",
    "MAX_FEATURES = 30000  # features for vectorizer (then apply SVD)\n",
    "SVD_COMPONENTS = 512  # reduce to this dimensionality (choose <= MAX_FEATURES)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 20          # early stopping patience (val loss)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "preproc = \"normalizacion\" \n",
    "#preproc = \"normalizacion_stopwords\" \n",
    "#preproc = \"normalizacion_stopwords_stem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c00afe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\envs\\RNA\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if USE_TFIDF:\n",
    "    vectorizer = Vec_TFID(modo_preproc=preproc)\n",
    "\n",
    "    X_tr = vectorizer.create_matriz_TFID(X_train, \n",
    "                                            ngram_config=NGRAM_RANGE, \n",
    "                                            max_config=MAX_FEATURES)\n",
    "    \n",
    "    X_val_vectorized = vectorizer.tranform_matriz_TFID(X_val)\n",
    "    X_t = vectorizer.tranform_matriz_TFID(X_test) \n",
    "else:\n",
    "    vectorizer = Vec_Count(modo_preproc=preproc)\n",
    "    X_tr = vectorizer.create_matriz_Count(X_train, \n",
    "                                            ngram_config=NGRAM_RANGE, \n",
    "                                            max_config=MAX_FEATURES)\n",
    "    \n",
    "    X_val_vectorized = vectorizer.transform_matriz_Count(X_val)\n",
    "    X_t = vectorizer.transform_matriz_Count(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "839c251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# NUM_CLASSES = 2\n",
    "# Y_train_one_hot = nn.functional.one_hot(torch.from_numpy(Y_train), num_classes=NUM_CLASSES).float()\n",
    "# Y_val_one_hot = nn.functional.one_hot(torch.from_numpy(Y_val), num_classes=NUM_CLASSES).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ce1e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    # Recibe los documentos en X y las etiquetas en Y\n",
    "    dataset = TensorDataset(X, Y) # Cargar los datos en un dataset de tensores\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3fbfdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9360,), (9360,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1434208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train_one_hot.shape,  Y_val_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8197dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # Definición de capas, funciones de activación e inicialización de pesos\n",
    "        input_size_h1 = 512\n",
    "        input_size_h2 = 128\n",
    "        input_size_h3 = 32\n",
    "        self.fc1 = nn.Linear(input_size, input_size_h1)\n",
    "        self.bn1 = nn.BatchNorm1d(input_size_h1)\n",
    "        # PReLU tiene parámetros aprendibles: Se recomienda una función de activación independiente por capa\n",
    "        self.act1= nn.LeakyReLU()\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(input_size_h1, input_size_h2)\n",
    "        self.bn2 = nn.BatchNorm1d(input_size_h2)\n",
    "        # PReLU tiene parámetros aprendibles: Se recomienda una función de activación independiente por capa\n",
    "        self.act2= nn.LeakyReLU()\n",
    "        self.drop2 = nn.Dropout(p=0.4)\n",
    "\n",
    "        self.fc3 = nn.Linear(input_size_h2, input_size_h3)\n",
    "        self.bn3 = nn.BatchNorm1d(input_size_h3)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.drop3 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.output = nn.Linear(input_size_h3, output_size)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "       \n",
    "    def forward(self, X):\n",
    "        # Definición del orden de conexión de las capas y aplición de las funciones de activación\n",
    "        x = self.fc1(X)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        # Nota la última capa de salida 'output' no se activa debido a que CrossEntropyLoss usa LogSoftmax internamente. \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4aeff3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Iniciando entrenamiento en PyTorch...\n",
      "Batch Error : 1.8663\n",
      "Época 1/50, Pérdida promedio: 1.4515\n",
      "--- Resultados Val Época 1 ---\n",
      "P = 0.6975329566854991\n",
      "R = 0.7088784804782559\n",
      "F1= 0.6745288757186543\n",
      "Acc= 0.6759615384615385\n",
      "--------------------------------------\n",
      "Época 2/50, Pérdida promedio: 0.7796\n",
      "--- Resultados Val Época 2 ---\n",
      "P = 0.7397688395746649\n",
      "R = 0.758195627670972\n",
      "F1= 0.7340463538552373\n",
      "Acc= 0.7384615384615385\n",
      "--------------------------------------\n",
      "Época 3/50, Pérdida promedio: 0.4599\n",
      "--- Resultados Val Época 3 ---\n",
      "P = 0.77035616417588\n",
      "R = 0.7840958423444414\n",
      "F1= 0.7745257452574525\n",
      "Acc= 0.7846153846153846\n",
      "--------------------------------------\n",
      "Batch Error : 0.1890\n",
      "Batch Error : 0.1401\n",
      "Época 4/50, Pérdida promedio: 0.2964\n",
      "--- Resultados Val Época 4 ---\n",
      "P = 0.7793979070574815\n",
      "R = 0.7916830957587054\n",
      "F1= 0.7836492660639642\n",
      "Acc= 0.7942307692307692\n",
      "--------------------------------------\n",
      "Época 5/50, Pérdida promedio: 0.1882\n",
      "--- Resultados Val Época 5 ---\n",
      "P = 0.7803052396228249\n",
      "R = 0.7871108296591909\n",
      "F1= 0.783223411242336\n",
      "Acc= 0.7961538461538461\n",
      "--------------------------------------\n",
      "Batch Error : 0.1516\n",
      "Batch Error : 0.1127\n",
      "Batch Error : 0.0713\n",
      "Época 6/50, Pérdida promedio: 0.1215\n",
      "--- Resultados Val Época 6 ---\n",
      "P = 0.7863290385511867\n",
      "R = 0.7911095710911704\n",
      "F1= 0.788500191518684\n",
      "Acc= 0.801923076923077\n",
      "--------------------------------------\n",
      "Batch Error : 0.0819\n",
      "Época 7/50, Pérdida promedio: 0.0852\n",
      "--- Resultados Val Época 7 ---\n",
      "P = 0.7832738458648383\n",
      "R = 0.7882797844503124\n",
      "F1= 0.7855308299431957\n",
      "Acc= 0.7990384615384616\n",
      "--------------------------------------\n",
      "Época 8/50, Pérdida promedio: 0.0712\n",
      "--- Resultados Val Época 8 ---\n",
      "P = 0.7924256819896855\n",
      "R = 0.7978763656060444\n",
      "F1= 0.7948717948717949\n",
      "Acc= 0.8076923076923077\n",
      "--------------------------------------\n",
      "Batch Error : 0.0223\n",
      "Época 9/50, Pérdida promedio: 0.0555\n",
      "--- Resultados Val Época 9 ---\n",
      "P = 0.7953071269088479\n",
      "R = 0.7886700998490515\n",
      "F1= 0.7916631014373998\n",
      "Acc= 0.8086538461538462\n",
      "--------------------------------------\n",
      "Batch Error : 0.0207\n",
      "Época 10/50, Pérdida promedio: 0.0424\n",
      "--- Resultados Val Época 10 ---\n",
      "P = 0.7958587398373984\n",
      "R = 0.7968308779308504\n",
      "F1= 0.79633688709958\n",
      "Acc= 0.8105769230769231\n",
      "--------------------------------------\n",
      "Batch Error : 0.0182\n",
      "Batch Error : 0.0255\n",
      "Época 11/50, Pérdida promedio: 0.0346\n",
      "--- Resultados Val Época 11 ---\n",
      "P = 0.7914175851547418\n",
      "R = 0.7982248614977756\n",
      "F1= 0.7943683446868159\n",
      "Acc= 0.8067307692307693\n",
      "--------------------------------------\n",
      "Época 12/50, Pérdida promedio: 0.0313\n",
      "--- Resultados Val Época 12 ---\n",
      "P = 0.802042819235942\n",
      "R = 0.7978564515550883\n",
      "F1= 0.7998195488721804\n",
      "Acc= 0.8153846153846154\n",
      "--------------------------------------\n",
      "Batch Error : 0.0138\n",
      "Época 13/50, Pérdida promedio: 0.0282\n",
      "--- Resultados Val Época 13 ---\n",
      "P = 0.8043289026725671\n",
      "R = 0.7993739022379411\n",
      "F1= 0.8016733310850959\n",
      "Acc= 0.8173076923076923\n",
      "--------------------------------------\n",
      "Época 14/50, Pérdida promedio: 0.0225\n",
      "--- Resultados Val Época 14 ---\n",
      "P = 0.7955094422485727\n",
      "R = 0.8001525416303235\n",
      "F1= 0.7976355919563901\n",
      "Acc= 0.8105769230769231\n",
      "--------------------------------------\n",
      "Batch Error : 0.0749\n",
      "Batch Error : 0.0161\n",
      "Época 15/50, Pérdida promedio: 0.0221\n",
      "--- Resultados Val Época 15 ---\n",
      "P = 0.8011781152545614\n",
      "R = 0.8011781152545614\n",
      "F1= 0.8011781152545614\n",
      "Acc= 0.8153846153846154\n",
      "--------------------------------------\n",
      "Batch Error : 0.0110\n",
      "Batch Error : 0.0056\n",
      "Época 16/50, Pérdida promedio: 0.0234\n",
      "--- Resultados Val Época 16 ---\n",
      "P = 0.795587679516251\n",
      "R = 0.7990453203971658\n",
      "F1= 0.7972109578139801\n",
      "Acc= 0.8105769230769231\n",
      "--------------------------------------\n",
      "Época 17/50, Pérdida promedio: 0.0231\n",
      "--- Resultados Val Época 17 ---\n",
      "P = 0.8004518072289157\n",
      "R = 0.7987585580633985\n",
      "F1= 0.7995828303433512\n",
      "Acc= 0.8144230769230769\n",
      "--------------------------------------\n",
      "Batch Error : 0.0171\n",
      "Batch Error : 0.0050\n",
      "Batch Error : 0.0076\n",
      "Batch Error : 0.0149\n",
      "Época 18/50, Pérdida promedio: 0.0229\n",
      "--- Resultados Val Época 18 ---\n",
      "P = 0.7991070539551297\n",
      "R = 0.7991070539551297\n",
      "F1= 0.7991070539551297\n",
      "Acc= 0.8134615384615385\n",
      "--------------------------------------\n",
      "Batch Error : 0.0045\n",
      "Época 19/50, Pérdida promedio: 0.0181\n",
      "--- Resultados Val Época 19 ---\n",
      "P = 0.805734335839599\n",
      "R = 0.8036594060036881\n",
      "F1= 0.8046643968094118\n",
      "Acc= 0.8192307692307692\n",
      "--------------------------------------\n",
      "Batch Error : 0.0089\n",
      "Época 20/50, Pérdida promedio: 0.0173\n",
      "--- Resultados Val Época 20 ---\n",
      "P = 0.806856321562204\n",
      "R = 0.8044181313451145\n",
      "F1= 0.8055929026615019\n",
      "Acc= 0.8201923076923077\n",
      "--------------------------------------\n",
      "Época 21/50, Pérdida promedio: 0.0163\n",
      "--- Resultados Val Época 21 ---\n",
      "P = 0.7956159034433794\n",
      "R = 0.8051350371795332\n",
      "F1= 0.7994469324327631\n",
      "Acc= 0.8105769230769231\n",
      "--------------------------------------\n",
      "Batch Error : 0.0252\n",
      "Época 22/50, Pérdida promedio: 0.0148\n",
      "--- Resultados Val Época 22 ---\n",
      "P = 0.795477148762257\n",
      "R = 0.8029205947132178\n",
      "F1= 0.7986617413086398\n",
      "Acc= 0.8105769230769231\n",
      "--------------------------------------\n",
      "Batch Error : 0.0092\n",
      "Batch Error : 0.0114\n",
      "Batch Error : 0.0057\n",
      "Época 23/50, Pérdida promedio: 0.0167\n",
      "--- Resultados Val Época 23 ---\n",
      "P = 0.7975000000000001\n",
      "R = 0.8033308241629129\n",
      "F1= 0.800102314495835\n",
      "Acc= 0.8125\n",
      "--------------------------------------\n",
      "Batch Error : 0.0050\n",
      "Batch Error : 0.0140\n",
      "Batch Error : 0.0126\n",
      "Batch Error : 0.0412\n",
      "Época 24/50, Pérdida promedio: 0.0166\n",
      "--- Resultados Val Época 24 ---\n",
      "P = 0.8078195488721804\n",
      "R = 0.8057304673031198\n",
      "F1= 0.8067424351412267\n",
      "Acc= 0.8211538461538461\n",
      "--------------------------------------\n",
      "Batch Error : 0.0049\n",
      "Batch Error : 0.0108\n",
      "Época 25/50, Pérdida promedio: 0.0155\n",
      "--- Resultados Val Época 25 ---\n",
      "P = 0.7965142926536897\n",
      "R = 0.8014648775883287\n",
      "F1= 0.7987671725129226\n",
      "Acc= 0.8115384615384615\n",
      "--------------------------------------\n",
      "Batch Error : 0.0122\n",
      "Época 26/50, Pérdida promedio: 0.0152\n",
      "--- Resultados Val Época 26 ---\n",
      "P = 0.8046174057933864\n",
      "R = 0.8029006806622617\n",
      "F1= 0.8037365540668051\n",
      "Acc= 0.8182692307692307\n",
      "--------------------------------------\n",
      "Época 27/50, Pérdida promedio: 0.0187\n",
      "--- Resultados Val Época 27 ---\n",
      "P = 0.8202785529518203\n",
      "R = 0.8003178282532589\n",
      "F1= 0.8080637818817131\n",
      "Acc= 0.8269230769230769\n",
      "--------------------------------------\n",
      "Batch Error : 0.0528\n",
      "Batch Error : 0.0390\n",
      "Época 28/50, Pérdida promedio: 0.0183\n",
      "--- Resultados Val Época 28 ---\n",
      "P = 0.8086573588379451\n",
      "R = 0.7964624679881631\n",
      "F1= 0.801603247082699\n",
      "Acc= 0.8192307692307692\n",
      "--------------------------------------\n",
      "Batch Error : 0.0081\n",
      "Época 29/50, Pérdida promedio: 0.0140\n",
      "--- Resultados Val Época 29 ---\n",
      "P = 0.8046174057933864\n",
      "R = 0.8029006806622617\n",
      "F1= 0.8037365540668051\n",
      "Acc= 0.8182692307692307\n",
      "--------------------------------------\n",
      "Batch Error : 0.0022\n",
      "Época 30/50, Pérdida promedio: 0.0134\n",
      "--- Resultados Val Época 30 ---\n",
      "P = 0.8041158536585367\n",
      "R = 0.8051151231285771\n",
      "F1= 0.8046074703645717\n",
      "Acc= 0.8182692307692307\n",
      "--------------------------------------\n",
      "Época 31/50, Pérdida promedio: 0.0116\n",
      "--- Resultados Val Época 31 ---\n",
      "P = 0.8040159401689088\n",
      "R = 0.8056687337451559\n",
      "F1= 0.8048201169530251\n",
      "Acc= 0.8182692307692307\n",
      "--------------------------------------\n",
      "Batch Error : 0.0076\n",
      "Batch Error : 0.0256\n",
      "Batch Error : 0.0080\n",
      "Época 32/50, Pérdida promedio: 0.0130\n",
      "--- Resultados Val Época 32 ---\n",
      "P = 0.7964920100563421\n",
      "R = 0.8020184882049076\n",
      "F1= 0.798974358974359\n",
      "Acc= 0.8115384615384615\n",
      "--------------------------------------\n",
      "Batch Error : 0.0079\n",
      "Batch Error : 0.0106\n",
      "Batch Error : 0.0077\n",
      "Batch Error : 0.0078\n",
      "Época 33/50, Pérdida promedio: 0.0134\n",
      "--- Resultados Val Época 33 ---\n",
      "P = 0.796495763320408\n",
      "R = 0.804232930671223\n",
      "F1= 0.799783124577643\n",
      "Acc= 0.8115384615384615\n",
      "--------------------------------------\n",
      "Época 34/50, Pérdida promedio: 0.0157\n",
      "--- Resultados Val Época 34 ---\n",
      "P = 0.8047686724157312\n",
      "R = 0.8023470700456828\n",
      "F1= 0.8035136823691116\n",
      "Acc= 0.8182692307692307\n",
      "--------------------------------------\n",
      "Batch Error : 0.0043\n",
      "Época 35/50, Pérdida promedio: 0.0150\n",
      "--- Resultados Val Época 35 ---\n",
      "P = 0.8062946577762218\n",
      "R = 0.8066325738114299\n",
      "F1= 0.8064627200904395\n",
      "Acc= 0.8201923076923077\n",
      "--------------------------------------\n",
      "Batch Error : 0.0067\n",
      "Época 36/50, Pérdida promedio: 0.0125\n",
      "--- Resultados Val Época 36 ---\n",
      "P = 0.8075878451784468\n",
      "R = 0.8022036888787991\n",
      "F1= 0.804690128355382\n",
      "Acc= 0.8201923076923077\n",
      "--------------------------------------\n",
      "Batch Error : 0.0065\n",
      "Batch Error : 0.0085\n",
      "Batch Error : 0.0037\n",
      "Batch Error : 0.0019\n",
      "Batch Error : 0.0078\n",
      "Época 37/50, Pérdida promedio: 0.0165\n",
      "--- Resultados Val Época 37 ---\n",
      "P = 0.8055858948353878\n",
      "R = 0.804213016620267\n",
      "F1= 0.8048850586183084\n",
      "Acc= 0.8192307692307692\n",
      "--------------------------------------\n",
      "Batch Error : 0.0097\n",
      "Época 38/50, Pérdida promedio: 0.0130\n",
      "--- Resultados Val Época 38 ---\n",
      "P = 0.805734335839599\n",
      "R = 0.8036594060036881\n",
      "F1= 0.8046643968094118\n",
      "Acc= 0.8192307692307692\n",
      "--------------------------------------\n",
      "Batch Error : 0.0097\n",
      "Época 39/50, Pérdida promedio: 0.0146\n",
      "--- Resultados Val Época 39 ---\n",
      "P = 0.8056749254651999\n",
      "R = 0.796052238538468\n",
      "F1= 0.8002393892339544\n",
      "Acc= 0.8173076923076923\n",
      "--------------------------------------\n",
      "Época 40/50, Pérdida promedio: 0.0136\n",
      "--- Resultados Val Época 40 ---\n",
      "P = 0.7985869885192818\n",
      "R = 0.8029823282711815\n",
      "F1= 0.8006134969325154\n",
      "Acc= 0.8134615384615385\n",
      "--------------------------------------\n",
      "Batch Error : 0.0056\n",
      "Batch Error : 0.0164\n",
      "Época 41/50, Pérdida promedio: 0.0158\n",
      "--- Resultados Val Época 41 ---\n",
      "P = 0.8070230506728479\n",
      "R = 0.8038645207285356\n",
      "F1= 0.8053703297836036\n",
      "Acc= 0.8201923076923077\n",
      "--------------------------------------\n",
      "Batch Error : 0.0059\n",
      "Época 42/50, Pérdida promedio: 0.0140\n",
      "--- Resultados Val Época 42 ---\n",
      "P = 0.797676282051282\n",
      "R = 0.8077597090955436\n",
      "F1= 0.8016724838174331\n",
      "Acc= 0.8125\n",
      "--------------------------------------\n",
      "Batch Error : 0.0038\n",
      "Batch Error : 0.0030\n",
      "Batch Error : 0.0108\n",
      "Época 43/50, Pérdida promedio: 0.0123\n",
      "--- Resultados Val Época 43 ---\n",
      "P = 0.8052028565637779\n",
      "R = 0.8058738484700034\n",
      "F1= 0.8055347793567689\n",
      "Acc= 0.8192307692307692\n",
      "--------------------------------------\n",
      "Batch Error : 0.0120\n",
      "Batch Error : 0.0130\n",
      "Época 44/50, Pérdida promedio: 0.0136\n",
      "--- Resultados Val Época 44 ---\n",
      "P = 0.8061274509803922\n",
      "R = 0.798471795729631\n",
      "F1= 0.8018945796155812\n",
      "Acc= 0.8182692307692307\n",
      "--------------------------------------\n",
      "Batch Error : 0.0354\n",
      "Época 45/50, Pérdida promedio: 0.0141\n",
      "--- Resultados Val Época 45 ---\n",
      "P = 0.8087505749392208\n",
      "R = 0.7994356357959049\n",
      "F1= 0.803514075193652\n",
      "Acc= 0.8201923076923077\n",
      "--------------------------------------\n",
      "Batch Error : 0.1512\n",
      "Batch Error : 0.0097\n",
      "Batch Error : 0.0052\n",
      "Época 46/50, Pérdida promedio: 0.0140\n",
      "--- Resultados Val Época 46 ---\n",
      "P = 0.8064194577352473\n",
      "R = 0.8060789631948511\n",
      "F1= 0.8062483125929631\n",
      "Acc= 0.8201923076923077\n",
      "--------------------------------------\n",
      "Época 47/50, Pérdida promedio: 0.0127\n",
      "--- Resultados Val Época 47 ---\n",
      "P = 0.7986869431231995\n",
      "R = 0.8018751070380239\n",
      "F1= 0.8001933101994898\n",
      "Acc= 0.8134615384615385\n",
      "--------------------------------------\n",
      "Batch Error : 0.0078\n",
      "Batch Error : 0.0229\n",
      "Batch Error : 0.0100\n",
      "Época 48/50, Pérdida promedio: 0.0135\n",
      "--- Resultados Val Época 48 ---\n",
      "P = 0.8021060305706188\n",
      "R = 0.7937760625141888\n",
      "F1= 0.797456909643703\n",
      "Acc= 0.8144230769230769\n",
      "--------------------------------------\n",
      "Batch Error : 0.0043\n",
      "Época 49/50, Pérdida promedio: 0.0127\n",
      "--- Resultados Val Época 49 ---\n",
      "P = 0.802940908785134\n",
      "R = 0.8049100084037295\n",
      "F1= 0.8038935467479675\n",
      "Acc= 0.8173076923076923\n",
      "--------------------------------------\n",
      "Batch Error : 0.0081\n",
      "Época 50/50, Pérdida promedio: 0.0150\n",
      "--- Resultados Val Época 50 ---\n",
      "P = 0.8030336076137603\n",
      "R = 0.8043563977871506\n",
      "F1= 0.8036807865485571\n",
      "Acc= 0.8173076923076923\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# --- 1. CONFIGURACIÓN DEL DISPOSITIVO (GPU vs CPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Parámetros de la red\n",
    "input_size = X_tr.shape[1]\n",
    "output_size = 2   # 2 clases\n",
    "\n",
    "\n",
    "# Conversión de datos a Tensores (Se quedan en CPU por ahora para no saturar VRAM)\n",
    "X_train_t = torch.from_numpy(X_tr).to(torch.float32)\n",
    "Y_train_t = torch.from_numpy(Y_train).long() # Asegúrate que Y_train sean índices (0, 1, 2...)\n",
    "\n",
    "X_val_t = torch.from_numpy(X_val_vectorized).to(torch.float32)\n",
    "# Y_val se queda como numpy para usarlo en sklearn al final\n",
    "\n",
    "# --- 2. CREAR LA RED Y MOVERLA A GPU ---\n",
    "model = MLP(input_size, output_size)\n",
    "model.to(device) # <--- IMPORTANTE: Mueve el modelo a la GPU\n",
    "\n",
    "# Calcular pesos de clases para el desbalance\n",
    "classes = np.unique(Y_train)\n",
    "weights_calc = compute_class_weight('balanced', classes=classes, y=Y_train)\n",
    "weights_calc[1] = weights_calc[1] * 1.3\n",
    "weights = torch.tensor(weights_calc).float()\n",
    "\n",
    "# --- 3. MOVER LOS PESOS DE LA LOSS A GPU ---\n",
    "weights = weights.to(device) # <--- IMPORTANTE: Los pesos deben estar en el mismo lugar que el modelo\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights) \n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "print(\"Iniciando entrenamiento en PyTorch...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  \n",
    "    lossTotal = 0\n",
    "    \n",
    "    # Asumo que create_minibatches devuelve iteradores de tensores\n",
    "    dataloader = create_minibatches(X_train_t, Y_train_t, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    for X_tra, y_tr in dataloader:\n",
    "        # --- 4. MOVER EL BATCH ACTUAL A GPU ---\n",
    "        X_tra = X_tra.to(device)\n",
    "        y_tr = y_tr.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Propagación hacia adelante\n",
    "        y_pred = model(X_tra)\n",
    "        \n",
    "        loss = criterion(y_pred, y_tr)\n",
    "        lossTotal += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if np.random.random() < 0.01: # Reduje la frecuencia de print para no ensuciar la consola\n",
    "            print(f\"Batch Error : {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Época {epoch+1}/{EPOCHS}, Pérdida promedio: {lossTotal/len(dataloader):.4f}\")\n",
    "    \n",
    "    # --- VALIDACIÓN ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Mover datos de validación a GPU\n",
    "        X_val_gpu = X_val_t.to(device)\n",
    "        \n",
    "        Y_val_tensor = torch.from_numpy(Y_val).long().to(device)\n",
    "        y_pred_logits = model(X_val_gpu)\n",
    "        \n",
    "        val_loss = criterion(y_pred_logits, Y_val_tensor)\n",
    "        # Softmax y Argmax\n",
    "        y_pred_prob = torch.softmax(y_pred_logits, dim=1)\n",
    "        y_pred_class = torch.argmax(y_pred_prob, dim=1)\n",
    "        \n",
    "        # --- 5. DEVOLVER A CPU PARA SKLEARN ---\n",
    "        # sklearn no funciona con tensores en GPU. \n",
    "        # Usamos .cpu() para bajarlo y .numpy() para convertirlo\n",
    "        y_pred_numpy = y_pred_class.cpu().numpy()\n",
    "        \n",
    "        print(f\"--- Resultados Val Época {epoch+1} ---\")\n",
    "        # Usamos 'binary' si solo hay 2 clases y el humor es la clase positiva (1)\n",
    "        # Usamos 'macro' si te importa el promedio de ambas\n",
    "        print(\"P =\", precision_score(Y_val, y_pred_numpy, average='macro'))\n",
    "        print(\"R =\", recall_score(Y_val, y_pred_numpy, average='macro'))\n",
    "        print(\"F1=\", f1_score(Y_val, y_pred_numpy, average='macro'))\n",
    "        print(\"Acc=\", accuracy_score(Y_val, y_pred_numpy))\n",
    "        print(\"--------------------------------------\")\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7d64f2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "X_val_t = torch.from_numpy(X_val_vectorized).to(torch.float32)\n",
    "\n",
    "# --- CRÍTICO: Mover los datos al mismo dispositivo que el modelo (GPU) ---\n",
    "# Si definiste 'device' anteriormente, úsalo aquí.\n",
    "device = next(model.parameters()).device # Truco para detectar dónde está el modelo automáticamente\n",
    "X_val_t = X_val_t.to(device)\n",
    "\n",
    "# 2. INFERENCIA\n",
    "model.eval() # Modo evaluación (apaga Dropout y Batch Norm)\n",
    "\n",
    "with torch.no_grad(): # Ahorra memoria y cálculo\n",
    "    # Predicción (Logits)\n",
    "    y_pred_logits = model(X_val_t)\n",
    "    \n",
    "    # Obtener la clase con mayor probabilidad (Argmax)\n",
    "    y_pred_class_tensor = torch.argmax(y_pred_logits, dim=1)\n",
    "\n",
    "# 3. POST-PROCESAMIENTO\n",
    "# --- CRÍTICO: Mover de GPU a CPU y convertir a Numpy para Scikit-Learn ---\n",
    "y_pred_val_numpy = y_pred_class_tensor.cpu().numpy()\n",
    "\n",
    "print(y_pred_val_numpy)\n",
    "# Y_val ya debería ser numpy (según tu código anterior), si es tensor, conviértelo también:\n",
    "# Y_val_numpy = Y_val.cpu().numpy() if torch.is_tensor(Y_val) else Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6a561d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Matriz de Confusión ---\n",
      "[[562  97]\n",
      " [ 93 288]]\n",
      "\n",
      "--- Reporte de Clasificación ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Humor     0.8580    0.8528    0.8554       659\n",
      "       Humor     0.7481    0.7559    0.7520       381\n",
      "\n",
      "    accuracy                         0.8173      1040\n",
      "   macro avg     0.8030    0.8044    0.8037      1040\n",
      "weighted avg     0.8177    0.8173    0.8175      1040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"--- Matriz de Confusión ---\")\n",
    "cm = confusion_matrix(Y_val, y_pred_val_numpy)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n--- Reporte de Clasificación ---\")\n",
    "# target_names ayuda a leer mejor el reporte\n",
    "print(classification_report(Y_val, y_pred_val_numpy, digits=4, zero_division='warn', target_names=['No Humor', 'Humor']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2ff001ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones generadas:\n",
      "[1 0 0 ... 0 0 1]\n",
      "\n",
      "Conteo de clases predichas:\n",
      "{np.int64(0): np.int64(3536), np.int64(1): np.int64(2064)}\n"
     ]
    }
   ],
   "source": [
    "# 1. PREPARAR DATOS\n",
    "# Convertir a tensor float\n",
    "X_testing = torch.from_numpy(X_t).to(torch.float32)\n",
    "\n",
    "# --- CRÍTICO: Detectar dispositivo y mover datos ---\n",
    "# Esto asegura que si el modelo está en GPU, los datos también vayan allá\n",
    "device = next(model.parameters()).device \n",
    "X_testing = X_testing.to(device)\n",
    "\n",
    "# 2. INFERENCIA\n",
    "model.eval() # Apaga Dropout y Batch Norm\n",
    "\n",
    "with torch.no_grad(): \n",
    "    # Predicción de logits\n",
    "    y_pred_test_logits = model(X_testing)\n",
    "\n",
    "# 3. PROCESAR RESULTADOS\n",
    "# Obtener la clase (0 o 1) usando argmax\n",
    "y_pred_test_indices = torch.argmax(y_pred_test_logits, dim=1)\n",
    "\n",
    "# --- CRÍTICO: Mover de vuelta a CPU y convertir a Numpy ---\n",
    "# Esto limpia la salida para que sea un array normal [0, 1, 0, 0...]\n",
    "y_pred_final = y_pred_test_indices.cpu().numpy()\n",
    "\n",
    "print(\"Predicciones generadas:\")\n",
    "print(y_pred_final)\n",
    "\n",
    "# OPCIONAL: Verificar distribución de predicciones\n",
    "unique, counts = np.unique(y_pred_final, return_counts=True)\n",
    "print(\"\\nConteo de clases predichas:\")\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "94aad3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_resultados(datos, archivo):\n",
    "\n",
    "    df = pd.DataFrame(datos, columns=['klass'])\n",
    "\n",
    "    df['id'] = df.index + 1\n",
    "\n",
    "    df = df[['id', 'klass']]\n",
    "\n",
    "    df.to_csv(archivo, index=False)\n",
    "\n",
    "    print(f\" {datos} guardado exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "67cfcc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1 0 0 ... 0 0 1] guardado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "guardar_resultados(y_pred_final, f\" neuronas{512}, {128}, {32}, {LEARNING_RATE}, {NGRAM_RANGE}, TFIDF {USE_TFIDF}, {BATCH_SIZE}, preprocesador {preproc}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
