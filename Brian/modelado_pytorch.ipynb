{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c0b8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de entrenamiento\n",
      "klass\n",
      "0    6588\n",
      "1    3812\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_train.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X_train = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y_train = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babc0687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de prueba\n",
      "klass\n",
      "-1    5600\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_test.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de prueba\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X_test = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y_test = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce8ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def eliminar_emojis(texto):\n",
    "    # Expresión regular que cubre la mayoría de emojis y pictogramas\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticonos\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # símbolos y pictogramas\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transporte y mapas\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # banderas\n",
    "        \"\\U00002702-\\U000027B0\"  # otros símbolos\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab414fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"english\")  # agregar más palabras a esta lista si es necesario\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "def mi_preprocesamiento_factory(modo):\n",
    "    \"\"\"\n",
    "    Devuelve una función de preprocesamiento y tokenización según el modo:\n",
    "    - 'normalizacion'\n",
    "    - 'normalizacion_stopwords'\n",
    "    - 'normalizacion_stopwords_stem'\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocesamiento(texto):\n",
    "        texto = texto.lower()\n",
    "        texto = eliminar_emojis(texto)\n",
    "        texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)\n",
    "        texto = re.sub(r\"@\\w+\", \"\", texto)\n",
    "        texto = normaliza_texto(texto)\n",
    "        return texto\n",
    "\n",
    "    def tokenizador(texto):\n",
    "        tokens = word_tokenize(texto)\n",
    "        if modo in ['normalizacion_stopwords', 'normalizacion_stopwords_stem']:\n",
    "            tokens = [t for t in tokens if t not in _STOPWORDS and len(t) > 2]\n",
    "        if modo == 'normalizacion_stopwords_stem':\n",
    "            tokens = [stemmer.stem(t) for t in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return preprocesamiento, tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f6a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class Vec_TFID:\n",
    "    def __init__(self, modo_preproc='normalizacion'):\n",
    "        self.vec_tfidf = None\n",
    "        self.modo_preproc = modo_preproc\n",
    "\n",
    "    def create_matriz_TFID(self, X_train, ngram_config, max_config):\n",
    "        preproc, token = mi_preprocesamiento_factory(self.modo_preproc)\n",
    "        self.vec_tfidf = TfidfVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            preprocessor=preproc,\n",
    "            tokenizer=token,\n",
    "            ngram_range=ngram_config,\n",
    "            max_features=max_config\n",
    "        )\n",
    "        X_tfidf = self.vec_tfidf.fit_transform(X_train)\n",
    "        return X_tfidf.toarray()\n",
    "\n",
    "    def tranform_matriz_TFID(self, X_test):\n",
    "        X_tfid = self.vec_tfidf.transform(X_test)\n",
    "        return X_tfid.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05546e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class Vec_Count:\n",
    "    def __init__(self, modo_preproc='normalizacion'):\n",
    "        self.vec = None\n",
    "        self.modo_preproc = modo_preproc\n",
    "\n",
    "    def create_matriz_Count(self, X_train, ngram_config, max_config):\n",
    "        preproc, token = mi_preprocesamiento_factory(self.modo_preproc)\n",
    "        self.vec = CountVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            preprocessor=preproc,\n",
    "            tokenizer=token,\n",
    "            ngram_range=ngram_config,\n",
    "            max_features=max_config\n",
    "        )\n",
    "        X_vec = self.vec.fit_transform(X_train)\n",
    "        return X_vec.toarray()\n",
    "\n",
    "    def transform_matriz_Count(self, X_test):\n",
    "        X_vec = self.vec.transform(X_test)\n",
    "        return X_vec.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50995330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
