{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c0b8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de entrenamiento\n",
      "klass\n",
      "0    6588\n",
      "1    3812\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_train.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X_train = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y_train = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babc0687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de prueba\n",
      "klass\n",
      "-1    5600\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_test.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de prueba\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X_test = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y_test = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028c4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide el conjunto de entrenamiento en:  entrenamiento (90%) y validación (10%)\n",
    "X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=0.1, stratify=Y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce8ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def eliminar_emojis(texto):\n",
    "    # Expresión regular que cubre la mayoría de emojis y pictogramas\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticonos\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # símbolos y pictogramas\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transporte y mapas\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # banderas\n",
    "        \"\\U00002702-\\U000027B0\"  # otros símbolos\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab414fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar más palabras a esta lista si es necesario\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "def mi_preprocesamiento_factory(modo):\n",
    "    \"\"\"\n",
    "    Devuelve una función de preprocesamiento y tokenización según el modo:\n",
    "    - 'normalizacion'\n",
    "    - 'normalizacion_stopwords'\n",
    "    - 'normalizacion_stopwords_stem'\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocesamiento(texto):\n",
    "        texto = texto.lower()\n",
    "        #texto = eliminar_emojis(texto)\n",
    "        texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)\n",
    "        texto = re.sub(r\"@\\w+\", \"\", texto)\n",
    "        texto = normaliza_texto(texto, punct=True)\n",
    "        return texto\n",
    "\n",
    "    def tokenizador(texto):\n",
    "        tokens = word_tokenize(texto)\n",
    "        if modo in ['normalizacion_stopwords', 'normalizacion_stopwords_stem']:\n",
    "            tokens = [t for t in tokens if t not in _STOPWORDS and len(t) > 2]\n",
    "        if modo == 'normalizacion_stopwords_stem':\n",
    "            tokens = [stemmer.stem(t) for t in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return preprocesamiento, tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f6a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class Vec_TFID:\n",
    "    def __init__(self, modo_preproc='normalizacion'):\n",
    "        self.vec_tfidf = None\n",
    "        self.modo_preproc = modo_preproc\n",
    "\n",
    "    def create_matriz_TFID(self, X_train, ngram_config, max_config):\n",
    "        preproc, token = mi_preprocesamiento_factory(self.modo_preproc)\n",
    "        self.vec_tfidf = TfidfVectorizer(\n",
    "            min_df=5,\n",
    "            analyzer=\"word\",\n",
    "            preprocessor=preproc,\n",
    "            tokenizer=token,\n",
    "            ngram_range=ngram_config,\n",
    "            max_features=max_config\n",
    "        )\n",
    "        X_tfidf = self.vec_tfidf.fit_transform(X_train)\n",
    "        return X_tfidf.toarray()\n",
    "\n",
    "    def tranform_matriz_TFID(self, X_test):\n",
    "        X_tfid = self.vec_tfidf.transform(X_test)\n",
    "        return X_tfid.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05546e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "class Vec_Count:\n",
    "    def __init__(self, modo_preproc='normalizacion'):\n",
    "        self.vec = None\n",
    "        self.modo_preproc = modo_preproc\n",
    "\n",
    "    def create_matriz_Count(self, X_train, ngram_config, max_config):\n",
    "        preproc, token = mi_preprocesamiento_factory(self.modo_preproc)\n",
    "        self.vec = CountVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            preprocessor=preproc,\n",
    "            tokenizer=token,\n",
    "            ngram_range=ngram_config,\n",
    "            max_features=max_config\n",
    "        )\n",
    "        X_vec = self.vec.fit_transform(X_train)\n",
    "        return X_vec.toarray().astype(np.float32)\n",
    "\n",
    "    def transform_matriz_Count(self, X_test):\n",
    "        X_vec = self.vec.transform(X_test)\n",
    "        return X_vec.toarray().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba8fe184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "USE_TFIDF = True      # True: TF-IDF, False: Count\n",
    "NGRAM_RANGE = (1, 2)\n",
    "MAX_FEATURES = 15000  # features for vectorizer \n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "preproc = \"normalizacion\" \n",
    "FACTOR_BALANCE = 2\n",
    "#preproc = \"normalizacion_stopwords\" \n",
    "#preproc = \"normalizacion_stopwords_stem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c00afe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\envs\\RNA\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if USE_TFIDF:\n",
    "    vectorizer = Vec_TFID(modo_preproc=preproc)\n",
    "\n",
    "    X_tr = vectorizer.create_matriz_TFID(X_train, \n",
    "                                            ngram_config=NGRAM_RANGE, \n",
    "                                            max_config=MAX_FEATURES)\n",
    "    \n",
    "    X_val_vectorized = vectorizer.tranform_matriz_TFID(X_val)\n",
    "    X_t = vectorizer.tranform_matriz_TFID(X_test) \n",
    "else:\n",
    "    vectorizer = Vec_Count(modo_preproc=preproc)\n",
    "    X_tr = vectorizer.create_matriz_Count(X_train, \n",
    "                                            ngram_config=NGRAM_RANGE, \n",
    "                                            max_config=MAX_FEATURES)\n",
    "    \n",
    "    X_val_vectorized = vectorizer.transform_matriz_Count(X_val)\n",
    "    X_t = vectorizer.transform_matriz_Count(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# NUM_CLASSES = 2\n",
    "# Y_train_one_hot = nn.functional.one_hot(torch.from_numpy(Y_train), num_classes=NUM_CLASSES).float()\n",
    "# Y_val_one_hot = nn.functional.one_hot(torch.from_numpy(Y_val), num_classes=NUM_CLASSES).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ce1e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    # Recibe los documentos en X y las etiquetas en Y\n",
    "    dataset = TensorDataset(X, Y) # Cargar los datos en un dataset de tensores\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fbfdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9360,), (9360,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1434208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train_one_hot.shape,  Y_val_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # Definición de capas, funciones de activación e inicialización de pesos\n",
    "        input_size_h1 = 1024\n",
    "        input_size_h2 = 256\n",
    "        input_size_h3 = 64\n",
    "        self.fc1 = nn.Linear(input_size, input_size_h1)\n",
    "        self.bn1 = nn.BatchNorm1d(input_size_h1)\n",
    "        \n",
    "        self.act1= nn.LeakyReLU(0.1)\n",
    "        self.drop1 = nn.Dropout(p=0.6)\n",
    "\n",
    "        self.fc2 = nn.Linear(input_size_h1, input_size_h2)\n",
    "        self.bn2 = nn.BatchNorm1d(input_size_h2)\n",
    "        \n",
    "        self.act2= nn.LeakyReLU(0.1)\n",
    "        self.drop2 = nn.Dropout(p=0.4)\n",
    "\n",
    "        self.fc3 = nn.Linear(input_size_h2, input_size_h3)\n",
    "        self.bn3 = nn.BatchNorm1d(input_size_h3)\n",
    "        self.act3 = nn.LeakyReLU(0.1)\n",
    "        self.drop3 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.output = nn.Linear(input_size_h3, output_size)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "       \n",
    "    def forward(self, X):\n",
    "        # Definición del orden de conexión de las capas y aplición de las funciones de activación\n",
    "        # x = self.fc1(X)\n",
    "        # x = self.bn1(x)\n",
    "        # x = self.act1(x)\n",
    "        # x = self.drop1(x)\n",
    "\n",
    "\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.bn2(x)\n",
    "        # x = self.act2(x)\n",
    "        # x = self.drop2(x)\n",
    "\n",
    "        \n",
    "        # x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.act3(x)\n",
    "        # x = self.drop3(x)\n",
    "\n",
    "        # x = self.output(x)\n",
    "\n",
    "        x = self.drop1(self.act1(self.bn1(self.fc1(X))))\n",
    "        x = self.drop2(self.act2(self.bn2(self.fc2(x))))\n",
    "        x = self.drop3(self.act3(self.bn3(self.fc3(x))))\n",
    "        x = self.output(x)\n",
    "\n",
    "        # Nota la última capa de salida 'output' no se activa debido a que CrossEntropyLoss usa LogSoftmax internamente. \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeff3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Iniciando entrenamiento en PyTorch...\n",
      "Batch Error : 2.3665\n",
      "Batch Error : 2.0409\n",
      "Batch Error : 1.9973\n",
      "Época 1/50, Pérdida promedio: 2.1793\n",
      "--- Resultados Val Época 1 ---\n",
      "P = 0.7485119047619048\n",
      "R = 0.7447675831112917\n",
      "F1= 0.7464989633059456\n",
      "Acc= 0.7663461538461539\n",
      "--------------------------------------\n",
      "Época 2/50, Pérdida promedio: 1.1198\n",
      "--- Resultados Val Época 2 ---\n",
      "P = 0.7620573467230444\n",
      "R = 0.7527650659752507\n",
      "F1= 0.7566822740849\n",
      "Acc= 0.7778846153846154\n",
      "--------------------------------------\n",
      "Época 3/50, Pérdida promedio: 0.8125\n",
      "--- Resultados Val Época 3 ---\n",
      "P = 0.7622606683737798\n",
      "R = 0.7410775094691313\n",
      "F1= 0.7484276729559749\n",
      "Acc= 0.775\n",
      "--------------------------------------\n",
      "Batch Error : 0.5315\n",
      "Época 4/50, Pérdida promedio: 0.6590\n",
      "--- Resultados Val Época 4 ---\n",
      "P = 0.7800982586194839\n",
      "R = 0.7552264426734215\n",
      "F1= 0.7637224298745475\n",
      "Acc= 0.7894230769230769\n",
      "--------------------------------------\n",
      "Batch Error : 0.2484\n",
      "Batch Error : 0.2263\n",
      "Batch Error : 0.6830\n",
      "Batch Error : 0.5420\n",
      "Época 5/50, Pérdida promedio: 0.5413\n",
      "--- Resultados Val Época 5 ---\n",
      "P = 0.7831143527139803\n",
      "R = 0.7615830077386002\n",
      "F1= 0.7693359123146357\n",
      "Acc= 0.7932692307692307\n",
      "--------------------------------------\n",
      "Batch Error : 0.3128\n",
      "Época 6/50, Pérdida promedio: 0.4366\n",
      "--- Resultados Val Época 6 ---\n",
      "P = 0.7840731672698886\n",
      "R = 0.7550830615065378\n",
      "F1= 0.7645239613146508\n",
      "Acc= 0.7913461538461538\n",
      "--------------------------------------\n",
      "Batch Error : 0.1412\n",
      "Época 7/50, Pérdida promedio: 0.3688\n",
      "--- Resultados Val Época 7 ---\n",
      "P = 0.7933957574861964\n",
      "R = 0.7609477495131014\n",
      "F1= 0.7712763283567083\n",
      "Acc= 0.7980769230769231\n",
      "--------------------------------------\n",
      "Batch Error : 0.3496\n",
      "Batch Error : 0.5860\n",
      "Época 8/50, Pérdida promedio: 0.3279\n",
      "--- Resultados Val Época 8 ---\n",
      "P = 0.7939210300352006\n",
      "R = 0.7603941388965226\n",
      "F1= 0.7709337595811361\n",
      "Acc= 0.7980769230769231\n",
      "--------------------------------------\n",
      "Batch Error : 0.2057\n",
      "Batch Error : 0.0815\n",
      "Batch Error : 0.3541\n",
      "Época 9/50, Pérdida promedio: 0.3002\n",
      "--- Resultados Val Época 9 ---\n",
      "P = 0.8024228994212879\n",
      "R = 0.7630805443704969\n",
      "F1= 0.7748468441067851\n",
      "Acc= 0.8028846153846154\n",
      "--------------------------------------\n",
      "Época 10/50, Pérdida promedio: 0.2756\n",
      "--- Resultados Val Época 10 ---\n",
      "P = 0.7857272617173308\n",
      "R = 0.763100458421453\n",
      "F1= 0.7711643532076968\n",
      "Acc= 0.7951923076923076\n",
      "--------------------------------------\n",
      "Época 11/50, Pérdida promedio: 0.2604\n",
      "--- Resultados Val Época 11 ---\n",
      "P = 0.8037086288416075\n",
      "R = 0.7619733231373392\n",
      "F1= 0.7741427328941194\n",
      "Acc= 0.8028846153846154\n",
      "--------------------------------------\n",
      "Batch Error : 0.3423\n",
      "Batch Error : 0.2372\n",
      "Época 12/50, Pérdida promedio: 0.2468\n",
      "--- Resultados Val Época 12 ---\n",
      "P = 0.800045045045045\n",
      "R = 0.7652949868368123\n",
      "F1= 0.7762194309129975\n",
      "Acc= 0.8028846153846154\n",
      "--------------------------------------\n",
      "Época 13/50, Pérdida promedio: 0.2361\n",
      "--- Resultados Val Época 13 ---\n",
      "P = 0.8035636442894507\n",
      "R = 0.7662588269030863\n",
      "F1= 0.7777288801902654\n",
      "Acc= 0.8048076923076923\n",
      "--------------------------------------\n",
      "Batch Error : 0.2096\n",
      "Batch Error : 0.1738\n",
      "Batch Error : 0.3515\n",
      "Época 14/50, Pérdida promedio: 0.2257\n",
      "--- Resultados Val Época 14 ---\n",
      "P = 0.7956053509332197\n",
      "R = 0.765438368003696\n",
      "F1= 0.7753753916688144\n",
      "Acc= 0.8009615384615385\n",
      "--------------------------------------\n",
      "Época 15/50, Pérdida promedio: 0.2142\n",
      "--- Resultados Val Época 15 ---\n",
      "P = 0.7985964691443497\n",
      "R = 0.7645362614953859\n",
      "F1= 0.7752969260653049\n",
      "Acc= 0.801923076923077\n",
      "--------------------------------------\n",
      "Batch Error : 0.2116\n",
      "Batch Error : 0.2313\n",
      "Época 16/50, Pérdida promedio: 0.2087\n",
      "--- Resultados Val Época 16 ---\n",
      "P = 0.8024091302609085\n",
      "R = 0.774071109093154\n",
      "F1= 0.7837577088297549\n",
      "Acc= 0.8076923076923077\n",
      "--------------------------------------\n",
      "Época 17/50, Pérdida promedio: 0.2054\n",
      "--- Resultados Val Época 17 ---\n",
      "P = 0.7983186234942815\n",
      "R = 0.7693753758777118\n",
      "F1= 0.779114124369076\n",
      "Acc= 0.8038461538461539\n",
      "--------------------------------------\n",
      "Batch Error : 0.1434\n",
      "Batch Error : 0.1659\n",
      "Época 18/50, Pérdida promedio: 0.2003\n",
      "--- Resultados Val Época 18 ---\n",
      "P = 0.7933021481210474\n",
      "R = 0.7682064210865902\n",
      "F1= 0.7769856389442247\n",
      "Acc= 0.8009615384615385\n",
      "--------------------------------------\n",
      "Batch Error : 0.1196\n",
      "Batch Error : 0.1126\n",
      "Batch Error : 0.2120\n",
      "Batch Error : 0.5393\n",
      "Época 19/50, Pérdida promedio: 0.1913\n",
      "--- Resultados Val Época 19 ---\n",
      "P = 0.7978387262635629\n",
      "R = 0.7699289864942906\n",
      "F1= 0.7794328630063501\n",
      "Acc= 0.8038461538461539\n",
      "--------------------------------------\n",
      "Batch Error : 0.1757\n",
      "Batch Error : 0.1061\n",
      "Época 20/50, Pérdida promedio: 0.1885\n",
      "--- Resultados Val Época 20 ---\n",
      "P = 0.8001239282622357\n",
      "R = 0.7720000477937223\n",
      "F1= 0.7815952859180525\n",
      "Acc= 0.8057692307692308\n",
      "--------------------------------------\n",
      "Batch Error : 0.1275\n",
      "Batch Error : 0.2244\n",
      "Batch Error : 0.2209\n",
      "Época 21/50, Pérdida promedio: 0.1785\n",
      "--- Resultados Val Época 21 ---\n",
      "P = 0.7989487986270023\n",
      "R = 0.76640220806997\n",
      "F1= 0.776888260994689\n",
      "Acc= 0.8028846153846154\n",
      "--------------------------------------\n",
      "Batch Error : 0.1106\n",
      "Batch Error : 0.2023\n",
      "Época 22/50, Pérdida promedio: 0.1876\n",
      "--- Resultados Val Época 22 ---\n",
      "P = 0.8074166761460585\n",
      "R = 0.7690886135439443\n",
      "F1= 0.7808459346920886\n",
      "Acc= 0.8076923076923077\n",
      "--------------------------------------\n",
      "Batch Error : 0.1367\n",
      "Época 23/50, Pérdida promedio: 0.1763\n",
      "--- Resultados Val Época 23 ---\n",
      "P = 0.7989487986270023\n",
      "R = 0.76640220806997\n",
      "F1= 0.776888260994689\n",
      "Acc= 0.8028846153846154\n",
      "--------------------------------------\n",
      "Época 24/50, Pérdida promedio: 0.1854\n",
      "--- Resultados Val Época 24 ---\n",
      "P = 0.8027101217584445\n",
      "R = 0.769231994710828\n",
      "F1= 0.7799896110859766\n",
      "Acc= 0.8057692307692308\n",
      "--------------------------------------\n",
      "Época 25/50, Pérdida promedio: 0.1762\n",
      "--- Resultados Val Época 25 ---\n",
      "P = 0.803849087601948\n",
      "R = 0.7681247734776704\n",
      "F1= 0.7793277310924369\n",
      "Acc= 0.8057692307692308\n",
      "--------------------------------------\n",
      "Batch Error : 0.2431\n",
      "Época 26/50, Pérdida promedio: 0.1727\n",
      "--- Resultados Val Época 26 ---\n",
      "P = 0.7896990856481985\n",
      "R = 0.7653766344457322\n",
      "F1= 0.7739130434782608\n",
      "Acc= 0.7980769230769231\n",
      "--------------------------------------\n",
      "Batch Error : 0.0901\n",
      "Época 27/50, Pérdida promedio: 0.1827\n",
      "--- Resultados Val Época 27 ---\n",
      "P = 0.7915502233545206\n",
      "R = 0.7680013063617428\n",
      "F1= 0.7763801537386443\n",
      "Acc= 0.8\n",
      "--------------------------------------\n",
      "Batch Error : 0.0673\n",
      "Época 28/50, Pérdida promedio: 0.1765\n",
      "--- Resultados Val Época 28 ---\n",
      "P = 0.8003815306903825\n",
      "R = 0.7671609334113965\n",
      "F1= 0.7778112904036596\n",
      "Acc= 0.8038461538461539\n",
      "--------------------------------------\n",
      "Batch Error : 0.0462\n",
      "Época 29/50, Pérdida promedio: 0.1850\n",
      "--- Resultados Val Época 29 ---\n",
      "P = 0.7994894021787899\n",
      "R = 0.7658485974533912\n",
      "F1= 0.7765552831784487\n",
      "Acc= 0.8028846153846154\n",
      "--------------------------------------\n",
      "Batch Error : 0.1651\n",
      "Época 30/50, Pérdida promedio: 0.1720\n",
      "--- Resultados Val Época 30 ---\n",
      "P = 0.8016317751776423\n",
      "R = 0.7703392159439857\n",
      "F1= 0.7806402031542369\n",
      "Acc= 0.8057692307692308\n",
      "--------------------------------------\n",
      "Batch Error : 0.1812\n",
      "Época 31/50, Pérdida promedio: 0.1685\n",
      "--- Resultados Val Época 31 ---\n",
      "P = 0.7938032660453468\n",
      "R = 0.7700723676611744\n",
      "F1= 0.7785303445680805\n",
      "Acc= 0.801923076923077\n",
      "--------------------------------------\n",
      "Época 32/50, Pérdida promedio: 0.1782\n",
      "--- Resultados Val Época 32 ---\n",
      "P = 0.8012728832951945\n",
      "R = 0.7684732693694016\n",
      "F1= 0.7790649608874238\n",
      "Acc= 0.8048076923076923\n",
      "--------------------------------------\n",
      "Batch Error : 0.1718\n",
      "Batch Error : 0.1160\n",
      "Batch Error : 0.0771\n",
      "Época 33/50, Pérdida promedio: 0.1743\n",
      "--- Resultados Val Época 33 ---\n",
      "P = 0.8003815306903825\n",
      "R = 0.7671609334113965\n",
      "F1= 0.7778112904036596\n",
      "Acc= 0.8038461538461539\n",
      "--------------------------------------\n",
      "Batch Error : 0.2459\n",
      "Época 34/50, Pérdida promedio: 0.1676\n",
      "--- Resultados Val Época 34 ---\n",
      "P = 0.8007381880999448\n",
      "R = 0.7690268799859805\n",
      "F1= 0.7793918647092815\n",
      "Acc= 0.8048076923076923\n",
      "--------------------------------------\n",
      "Batch Error : 0.3519\n",
      "Época 35/50, Pérdida promedio: 0.1781\n",
      "--- Resultados Val Época 35 ---\n",
      "P = 0.7987445386113596\n",
      "R = 0.7712413224522958\n",
      "F1= 0.7806716359764174\n",
      "Acc= 0.8048076923076923\n",
      "--------------------------------------\n",
      "Batch Error : 0.1857\n",
      "Batch Error : 0.2197\n",
      "Época 36/50, Pérdida promedio: 0.1630\n",
      "--- Resultados Val Época 36 ---\n",
      "P = 0.8050506199522238\n",
      "R = 0.7670175522445126\n",
      "F1= 0.7786543940390094\n",
      "Acc= 0.8057692307692308\n",
      "--------------------------------------\n",
      "Batch Error : 0.5915\n",
      "Batch Error : 0.1174\n",
      "Época 37/50, Pérdida promedio: 0.1755\n",
      "--- Resultados Val Época 37 ---\n",
      "P = 0.8044419067299282\n",
      "R = 0.7675711628610915\n",
      "F1= 0.7789925054389221\n",
      "Acc= 0.8057692307692308\n",
      "--------------------------------------\n",
      "Batch Error : 0.0831\n",
      "Batch Error : 0.0975\n",
      "Época 38/50, Pérdida promedio: 0.1668\n",
      "--- Resultados Val Época 38 ---\n",
      "P = 0.8020854575363323\n",
      "R = 0.7655001015616598\n",
      "F1= 0.7768043124234658\n",
      "Acc= 0.8038461538461539\n",
      "--------------------------------------\n",
      "Batch Error : 0.2126\n",
      "Época 39/50, Pérdida promedio: 0.1663\n",
      "--- Resultados Val Época 39 ---\n",
      "P = 0.7964634766893018\n",
      "R = 0.7691702611528642\n",
      "F1= 0.7785107653949043\n",
      "Acc= 0.8028846153846154\n",
      "--------------------------------------\n",
      "Batch Error : 0.2035\n",
      "Época 40/50, Pérdida promedio: 0.1697\n",
      "--- Resultados Val Época 40 ---\n",
      "P = 0.7920804883095386\n",
      "R = 0.7698672529363269\n",
      "F1= 0.7779187620889749\n",
      "Acc= 0.8009615384615385\n",
      "--------------------------------------\n",
      "Época 41/50, Pérdida promedio: 0.1709\n",
      "--- Resultados Val Época 41 ---\n",
      "P = 0.7984230022617584\n",
      "R = 0.7669558186865488\n",
      "F1= 0.7772183855438557\n",
      "Acc= 0.8028846153846154\n",
      "--------------------------------------\n",
      "Batch Error : 0.0727\n",
      "Época 42/50, Pérdida promedio: 0.1701\n",
      "--- Resultados Val Época 42 ---\n",
      "P = 0.8015020829666138\n",
      "R = 0.7660537121782387\n",
      "F1= 0.7771428571428571\n",
      "Acc= 0.8038461538461539\n",
      "--------------------------------------\n",
      "Época 43/50, Pérdida promedio: 0.1646\n",
      "--- Resultados Val Época 43 ---\n",
      "P = 0.7987445386113596\n",
      "R = 0.7712413224522958\n",
      "F1= 0.7806716359764174\n",
      "Acc= 0.8048076923076923\n",
      "--------------------------------------\n",
      "Batch Error : 0.1821\n",
      "Batch Error : 0.0683\n",
      "Batch Error : 0.1822\n",
      "Batch Error : 0.1290\n",
      "Época 44/50, Pérdida promedio: 0.1684\n",
      "--- Resultados Val Época 44 ---\n",
      "P = 0.8056096278080732\n",
      "R = 0.7707494453936808\n",
      "F1= 0.781841675791558\n",
      "Acc= 0.8076923076923077\n",
      "--------------------------------------\n",
      "Batch Error : 0.1152\n",
      "Batch Error : 0.4170\n",
      "Batch Error : 0.0534\n",
      "Época 45/50, Pérdida promedio: 0.1561\n",
      "--- Resultados Val Época 45 ---\n",
      "P = 0.8047297297297298\n",
      "R = 0.7694371094356756\n",
      "F1= 0.7805858810415243\n",
      "Acc= 0.8067307692307693\n",
      "--------------------------------------\n",
      "Época 46/50, Pérdida promedio: 0.1668\n",
      "--- Resultados Val Época 46 ---\n",
      "P = 0.8056096278080732\n",
      "R = 0.7707494453936808\n",
      "F1= 0.781841675791558\n",
      "Acc= 0.8076923076923077\n",
      "--------------------------------------\n",
      "Batch Error : 0.0815\n",
      "Batch Error : 0.1364\n",
      "Época 47/50, Pérdida promedio: 0.1743\n",
      "--- Resultados Val Época 47 ---\n",
      "P = 0.8006122115923235\n",
      "R = 0.7714464371771435\n",
      "F1= 0.7812796721693791\n",
      "Acc= 0.8057692307692308\n",
      "--------------------------------------\n",
      "Batch Error : 0.1409\n",
      "Batch Error : 0.2717\n",
      "Batch Error : 0.1953\n",
      "Batch Error : 0.2490\n",
      "Batch Error : 0.1605\n",
      "Época 48/50, Pérdida promedio: 0.1762\n",
      "--- Resultados Val Época 48 ---\n",
      "P = 0.7955669959406959\n",
      "R = 0.7702774823860219\n",
      "F1= 0.7791403670703674\n",
      "Acc= 0.8028846153846154\n",
      "--------------------------------------\n",
      "Batch Error : 0.0809\n",
      "Época 49/50, Pérdida promedio: 0.1821\n",
      "--- Resultados Val Época 49 ---\n",
      "P = 0.7855924486681287\n",
      "R = 0.7660736262291947\n",
      "F1= 0.7733319625783901\n",
      "Acc= 0.7961538461538461\n",
      "--------------------------------------\n",
      "Batch Error : 0.1390\n",
      "Batch Error : 0.2327\n",
      "Época 50/50, Pérdida promedio: 0.1756\n",
      "--- Resultados Val Época 50 ---\n",
      "P = 0.7955669959406959\n",
      "R = 0.7702774823860219\n",
      "F1= 0.7791403670703674\n",
      "Acc= 0.8028846153846154\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# --- 1. CONFIGURACIÓN DEL DISPOSITIVO (GPU vs CPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Parámetros de la red\n",
    "input_size = X_tr.shape[1]\n",
    "output_size = 2   # 2 clases\n",
    "\n",
    "\n",
    "\n",
    "X_train_t = torch.from_numpy(X_tr).to(torch.float32)\n",
    "Y_train_t = torch.from_numpy(Y_train).long() \n",
    "\n",
    "X_val_t = torch.from_numpy(X_val_vectorized).to(torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "model = MLP(input_size, output_size)\n",
    "model.to(device) \n",
    "\n",
    "# Calcular pesos de clases para el desbalance\n",
    "classes = np.unique(Y_train)\n",
    "weights_calc = compute_class_weight('balanced', classes=classes, y=Y_train)\n",
    "weights_calc[1] = weights_calc[1] * FACTOR_BALANCE\n",
    "weights = torch.tensor(weights_calc).float()\n",
    "\n",
    "\n",
    "weights = weights.to(device) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights) \n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "print(\"Iniciando entrenamiento en PyTorch...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  \n",
    "    lossTotal = 0\n",
    "    \n",
    "    \n",
    "    dataloader = create_minibatches(X_train_t, Y_train_t, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    for X_tra, y_tr in dataloader:\n",
    "        # --- 4. MOVER EL BATCH ACTUAL A GPU ---\n",
    "        X_tra = X_tra.to(device)\n",
    "        y_tr = y_tr.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Propagación hacia adelante\n",
    "        y_pred = model(X_tra)\n",
    "        \n",
    "        loss = criterion(y_pred, y_tr)\n",
    "        lossTotal += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if np.random.random() < 0.01: \n",
    "            print(f\"Batch Error : {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Época {epoch+1}/{EPOCHS}, Pérdida promedio: {lossTotal/len(dataloader):.4f}\")\n",
    "    \n",
    "    # --- VALIDACIÓN ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Mover datos de validación a GPU\n",
    "        X_val_gpu = X_val_t.to(device)\n",
    "        \n",
    "        Y_val_tensor = torch.from_numpy(Y_val).long().to(device)\n",
    "        y_pred_logits = model(X_val_gpu)\n",
    "        \n",
    "        val_loss = criterion(y_pred_logits, Y_val_tensor)\n",
    "        # Softmax y Argmax\n",
    "        y_pred_prob = torch.softmax(y_pred_logits, dim=1)\n",
    "        y_pred_class = torch.argmax(y_pred_prob, dim=1)\n",
    "        \n",
    "\n",
    "        y_pred_numpy = y_pred_class.cpu().numpy()\n",
    "        \n",
    "        print(f\"--- Resultados Val Época {epoch+1} ---\")\n",
    "        # Usamos 'binary' si solo hay 2 clases y el humor es la clase positiva (1)\n",
    "        # Usamos 'macro' si te importa el promedio de ambas\n",
    "        print(\"P =\", precision_score(Y_val, y_pred_numpy, average='macro'))\n",
    "        print(\"R =\", recall_score(Y_val, y_pred_numpy, average='macro'))\n",
    "        print(\"F1=\", f1_score(Y_val, y_pred_numpy, average='macro'))\n",
    "        print(\"Acc=\", accuracy_score(Y_val, y_pred_numpy))\n",
    "        print(\"--------------------------------------\")\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64f2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "X_val_t = torch.from_numpy(X_val_vectorized).to(torch.float32)\n",
    "\n",
    "\n",
    "device = next(model.parameters()).device # Truco para detectar dónde está el modelo automáticamente\n",
    "X_val_t = X_val_t.to(device)\n",
    "\n",
    "# 2. INFERENCIA\n",
    "model.eval() # Modo evaluación (apaga Dropout y Batch Norm)\n",
    "\n",
    "with torch.no_grad(): # Ahorra memoria y cálculo\n",
    "    # Predicción (Logits)\n",
    "    y_pred_logits = model(X_val_t)\n",
    "    \n",
    "    # Obtener la clase con mayor probabilidad (Argmax)\n",
    "    y_pred_class_tensor = torch.argmax(y_pred_logits, dim=1)\n",
    "\n",
    "# 3. POST-PROCESAMIENTO\n",
    "\n",
    "y_pred_val_numpy = y_pred_class_tensor.cpu().numpy()\n",
    "\n",
    "print(y_pred_val_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a561d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Matriz de Confusión ---\n",
      "[[556 103]\n",
      " [ 99 282]]\n",
      "\n",
      "--- Reporte de Clasificación ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Humor     0.8489    0.8437    0.8463       659\n",
      "       Humor     0.7325    0.7402    0.7363       381\n",
      "\n",
      "    accuracy                         0.8058      1040\n",
      "   macro avg     0.7907    0.7919    0.7913      1040\n",
      "weighted avg     0.8062    0.8058    0.8060      1040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"--- Matriz de Confusión ---\")\n",
    "cm = confusion_matrix(Y_val, y_pred_val_numpy)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n--- Reporte de Clasificación ---\")\n",
    "\n",
    "print(classification_report(Y_val, y_pred_val_numpy, digits=4, zero_division='warn', target_names=['No Humor', 'Humor']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff001ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones generadas:\n",
      "[1 0 0 ... 0 0 1]\n",
      "\n",
      "Conteo de clases predichas:\n",
      "{np.int64(0): np.int64(3531), np.int64(1): np.int64(2069)}\n"
     ]
    }
   ],
   "source": [
    "# 1. PREPARAR DATOS\n",
    "# Convertir a tensor float\n",
    "X_testing = torch.from_numpy(X_t).to(torch.float32)\n",
    "\n",
    "\n",
    "device = next(model.parameters()).device \n",
    "X_testing = X_testing.to(device)\n",
    "\n",
    "# 2. INFERENCIA\n",
    "model.eval() # Apaga Dropout y Batch Norm\n",
    "\n",
    "with torch.no_grad(): \n",
    "    # Predicción de logits\n",
    "    y_pred_test_logits = model(X_testing)\n",
    "\n",
    "# 3. PROCESAR RESULTADOS\n",
    "# Obtener la clase (0 o 1) usando argmax\n",
    "y_pred_test_indices = torch.argmax(y_pred_test_logits, dim=1)\n",
    "\n",
    "\n",
    "y_pred_final = y_pred_test_indices.cpu().numpy()\n",
    "\n",
    "print(\"Predicciones generadas:\")\n",
    "print(y_pred_final)\n",
    "\n",
    "\n",
    "unique, counts = np.unique(y_pred_final, return_counts=True)\n",
    "print(\"\\nConteo de clases predichas:\")\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94aad3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_resultados(datos, archivo):\n",
    "\n",
    "    df = pd.DataFrame(datos, columns=['klass'])\n",
    "\n",
    "    df['id'] = df.index + 1\n",
    "\n",
    "    df = df[['id', 'klass']]\n",
    "\n",
    "    df.to_csv(archivo, index=False)\n",
    "\n",
    "    print(f\" {datos} guardado exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67cfcc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1 0 0 ... 0 0 1] guardado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "guardar_resultados(y_pred_final, f\" neuronas{1024}, {256}, {64}, {LEARNING_RATE}, {NGRAM_RANGE}, TFIDF {USE_TFIDF}, {BATCH_SIZE}, preprocesador {preproc}, {MAX_FEATURES}, balance {FACTOR_BALANCE}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
