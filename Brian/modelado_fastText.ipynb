{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cdbb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FusionBiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, n_layers, \n",
    "                 embedding_matrix_mx, embedding_matrix_es, embedding_matrix_neu):\n",
    "        super(FusionBiLSTMAttention, self).__init__()\n",
    "        \n",
    "        # 1. Capas de Embedding (Congeladas o Fine-tuning opcional)\n",
    "        # Asumimos que recibimos las matrices listas como tensores\n",
    "        self.embed_mx = nn.Embedding.from_pretrained(embedding_matrix_mx, freeze=False)\n",
    "        self.embed_es = nn.Embedding.from_pretrained(embedding_matrix_es, freeze=False)\n",
    "        self.embed_neu = nn.Embedding.from_pretrained(embedding_matrix_neu, freeze=False)\n",
    "        \n",
    "        # Dimensión fusionada: 300 + 300 + 300 = 900\n",
    "        fusion_dim = 300 * 3\n",
    "        \n",
    "        # 2. LSTM Bidireccional\n",
    "        self.lstm = nn.LSTM(fusion_dim, hidden_dim, num_layers=n_layers, \n",
    "                            bidirectional=True, batch_first=True, dropout=0.5)\n",
    "        \n",
    "        # 3. Mecanismo de Atención\n",
    "        self.attention_linear = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "        # 4. Clasificador Final\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def attention(self, lstm_output, mask):\n",
    "        # lstm_output: [batch, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Calcular pesos de atención\n",
    "        # weights: [batch, seq_len, 1]\n",
    "        weights = torch.tanh(self.attention_linear(lstm_output))\n",
    "        \n",
    "        # Aplicar máscara (ignorar padding) - asumiendo 0 es pad\n",
    "        if mask is not None:\n",
    "            weights = weights.masked_fill(mask.unsqueeze(-1) == 0, -1e9)\n",
    "            \n",
    "        attn_weights = F.softmax(weights, dim=1)\n",
    "        \n",
    "        # Context vector: suma ponderada\n",
    "        # context: [batch, hidden_dim * 2]\n",
    "        context = torch.sum(attn_weights * lstm_output, dim=1)\n",
    "        return context\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: [batch, seq_len] (índices de palabras)\n",
    "        \n",
    "        # Fusión: Obtener vectores de los 3 dominios y concatenar\n",
    "        v_mx = self.embed_mx(x)   # [batch, seq, 300]\n",
    "        v_es = self.embed_es(x)   # [batch, seq, 300]\n",
    "        v_neu = self.embed_neu(x) # [batch, seq, 300]\n",
    "        \n",
    "        # Concatenación (Early Fusion) -> [batch, seq, 900]\n",
    "        fused_embed = torch.cat((v_mx, v_es, v_neu), dim=2)\n",
    "        \n",
    "        # Paso por LSTM\n",
    "        lstm_out, _ = self.lstm(fused_embed)\n",
    "        \n",
    "        # Paso por Atención\n",
    "        context_vector = self.attention(lstm_out, mask)\n",
    "        \n",
    "        # Clasificación\n",
    "        out = self.dropout(context_vector)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315ec733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_train_embeddings.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88cdb853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construyendo vocabulario desde la columna 'text'...\n",
      "Tamaño del vocabulario (vocab_size): 30731\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Limpieza básica (ajusta según tus necesidades)\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '<URL>', text) # Reemplazar URLs\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)   # Reemplazar Usuarios\n",
    "    return text.split() # Divide por espacios\n",
    "\n",
    "# 1. Construir el Vocabulario (Mapeo Palabra -> Índice)\n",
    "print(\"Construyendo vocabulario desde la columna 'text'...\")\n",
    "all_words = []\n",
    "for text in dataset['text']:\n",
    "    all_words.extend(tokenizer(text))\n",
    "\n",
    "# Contamos las palabras y filtramos las muy raras (opcional, min_freq=1)\n",
    "word_counts = Counter(all_words)\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "# Crear diccionarios\n",
    "word_to_idx = {'<PAD>': 0, '<UNK>': 1} # 0 para relleno, 1 para desconocidas\n",
    "for idx, word in enumerate(sorted_vocab, start=2):\n",
    "    word_to_idx[word] = idx\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"Tamaño del vocabulario (vocab_size): {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7458e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURACIÓN ---\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Supongamos que ya cargaste tus jsons y creaste las matrices de embedding\n",
    "# embeddings_mx = ... (Tensor shape: [Vocab_Size, 300])\n",
    "# embeddings_es = ...\n",
    "# embeddings_neu = ...\n",
    "\n",
    "model = FusionBiLSTMAttention(\n",
    "    vocab_size=len(vocabulario), # Tu tamaño de vocabulario\n",
    "    hidden_dim=128,              # 128 suele ser suficiente para tweets\n",
    "    n_layers=2,\n",
    "    embedding_matrix_mx=we_mx,\n",
    "    embedding_matrix_es=we_es,\n",
    "    embedding_matrix_neu=we_ft\n",
    ").to(DEVICE)\n",
    "\n",
    "# --- MANEJO DE DESBALANCE ---\n",
    "# Calculamos el pos_weight para la clase positiva (1)\n",
    "pos_weight = torch.tensor([6588 / 3812]).to(DEVICE) # ~1.72\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# --- BUCLE DE ENTRENAMIENTO (Simplificado) ---\n",
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, labels = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        # mask crea un tensor de 1s donde hay palabras y 0 donde hay padding\n",
    "        mask = (text != 0).type(torch.bool) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(text, mask).squeeze(1)\n",
    "        loss = criterion(predictions, labels.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Convertir logits a predicción 0 o 1 usando Sigmoid\n",
    "        preds_binary = torch.round(torch.sigmoid(predictions)).detach().cpu().numpy()\n",
    "        all_preds.extend(preds_binary)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return epoch_loss / len(iterator), f1\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# for epoch in range(10):\n",
    "#     train_loss, train_f1 = train_epoch(model, train_loader, optimizer, criterion)\n",
    "#     print(f'Epoch {epoch+1}: F1 Score Train = {train_f1:.4f}')\n",
    "#     # Aquí validarías con tu set de prueba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
