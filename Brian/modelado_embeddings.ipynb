{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset...\n",
      "Procesando embeddings...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Cargar el dataset\n",
    "print(\"Cargando dataset...\")\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_train_embeddings.json\", lines=True)\n",
    "\n",
    "# 2. Extraer y concatenar los embeddings\n",
    "print(\"Procesando embeddings...\")\n",
    "we_ft = np.stack(dataset['we_ft'].values) # (N, 300)\n",
    "we_mx = np.stack(dataset['we_mx'].values) # (N, 300)\n",
    "we_es = np.stack(dataset['we_es'].values) # (N, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976abce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión final de entrada: (10400, 900)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_full = np.concatenate([we_ft, we_mx, we_es], axis=1) \n",
    "\n",
    "Y_full = dataset['klass'].to_numpy()\n",
    "\n",
    "print(f\"Dimensión final de entrada: {X_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8b0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (8840, 900), Val: (1560, 900)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Dividir en Train y Validation\n",
    "# Usamos stratify para mantener la proporción de humor/no humor\n",
    "X_tr, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_full, Y_full, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=Y_full\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_tr.shape}, Val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        input_size_h1 = 512\n",
    "        input_size_h2 = 128\n",
    "        input_size_h3 = 64\n",
    "\n",
    "        # Capa 1: Recibe muchas características\n",
    "        self.fc1 = nn.Linear(input_size, input_size_h1)\n",
    "        self.bn1 = nn.BatchNorm1d(input_size_h1)\n",
    "        self.act1 = nn.LeakyReLU(0.1)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Capa 2: Compresión\n",
    "        self.fc2 = nn.Linear(input_size_h1, input_size_h2)\n",
    "        self.bn2 = nn.BatchNorm1d(input_size_h2)\n",
    "        self.act2 = nn.LeakyReLU(0.1)\n",
    "        self.drop2 = nn.Dropout(0.4)\n",
    "\n",
    "        # Capa 3: Refinamiento\n",
    "        self.fc3 = nn.Linear(input_size_h2, input_size_h3)\n",
    "        self.bn3 = nn.BatchNorm1d(input_size_h3)\n",
    "        self.act3 = nn.LeakyReLU(0.1)\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "\n",
    "        # Salida\n",
    "        self.output = nn.Linear(input_size_h3, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.act1(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(self.act2(self.bn2(self.fc2(x))))\n",
    "        x = self.drop3(self.act3(self.bn3(self.fc3(x))))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d59360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    # Recibe los documentos en X y las etiquetas en Y\n",
    "    dataset = TensorDataset(X, Y) # Cargar los datos en un dataset de tensores\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0005 \n",
    "EPOCHS = 100 \n",
    "FACTOR_BALANCE = 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2b41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Entrenamiento Fusión (900 inputs) ---\n",
      "Ep 1 | Val Loss: 0.4239 | F1 Macro: 0.7854 (Humor F1: 0.7508)\n",
      "  ★ ¡Nuevo Récord! Modelo guardado.\n",
      "Ep 2 | Val Loss: 0.4218 | F1 Macro: 0.7796 (Humor F1: 0.7483)\n",
      "Ep 3 | Val Loss: 0.4691 | F1 Macro: 0.7265 (Humor F1: 0.7047)\n",
      "Ep 4 | Val Loss: 0.4434 | F1 Macro: 0.8033 (Humor F1: 0.7496)\n",
      "  ★ ¡Nuevo Récord! Modelo guardado.\n",
      "Ep 5 | Val Loss: 0.4693 | F1 Macro: 0.7461 (Humor F1: 0.7234)\n",
      "Ep 6 | Val Loss: 0.4421 | F1 Macro: 0.7940 (Humor F1: 0.7520)\n",
      "Ep 7 | Val Loss: 0.4660 | F1 Macro: 0.8029 (Humor F1: 0.7534)\n",
      "Ep 8 | Val Loss: 0.4744 | F1 Macro: 0.7782 (Humor F1: 0.7426)\n",
      "Ep 9 | Val Loss: 0.4730 | F1 Macro: 0.7981 (Humor F1: 0.7527)\n",
      "Ep 10 | Val Loss: 0.5149 | F1 Macro: 0.7961 (Humor F1: 0.7518)\n",
      "Ep 11 | Val Loss: 0.5225 | F1 Macro: 0.7994 (Humor F1: 0.7571)\n",
      "Ep 12 | Val Loss: 0.5280 | F1 Macro: 0.8044 (Humor F1: 0.7580)\n",
      "  ★ ¡Nuevo Récord! Modelo guardado.\n",
      "Ep 13 | Val Loss: 0.5313 | F1 Macro: 0.7828 (Humor F1: 0.7441)\n",
      "Ep 14 | Val Loss: 0.5189 | F1 Macro: 0.8012 (Humor F1: 0.7558)\n",
      "Ep 15 | Val Loss: 0.5304 | F1 Macro: 0.8003 (Humor F1: 0.7572)\n",
      "Ep 16 | Val Loss: 0.5441 | F1 Macro: 0.7883 (Humor F1: 0.7476)\n",
      "Ep 17 | Val Loss: 0.5537 | F1 Macro: 0.7924 (Humor F1: 0.7510)\n",
      "Ep 18 | Val Loss: 0.5803 | F1 Macro: 0.7848 (Humor F1: 0.7481)\n",
      "Ep 19 | Val Loss: 0.5743 | F1 Macro: 0.7874 (Humor F1: 0.7458)\n",
      "Ep 20 | Val Loss: 0.5629 | F1 Macro: 0.7964 (Humor F1: 0.7527)\n",
      "Ep 21 | Val Loss: 0.6254 | F1 Macro: 0.8124 (Humor F1: 0.7612)\n",
      "  ★ ¡Nuevo Récord! Modelo guardado.\n",
      "Ep 22 | Val Loss: 0.6080 | F1 Macro: 0.7980 (Humor F1: 0.7537)\n",
      "Ep 23 | Val Loss: 0.6057 | F1 Macro: 0.8088 (Humor F1: 0.7601)\n",
      "Ep 24 | Val Loss: 0.5638 | F1 Macro: 0.7918 (Humor F1: 0.7522)\n",
      "Ep 25 | Val Loss: 0.5858 | F1 Macro: 0.7908 (Humor F1: 0.7518)\n",
      "Ep 26 | Val Loss: 0.5981 | F1 Macro: 0.7932 (Humor F1: 0.7524)\n",
      "Ep 27 | Val Loss: 0.6205 | F1 Macro: 0.7993 (Humor F1: 0.7553)\n",
      "Ep 28 | Val Loss: 0.6027 | F1 Macro: 0.8019 (Humor F1: 0.7568)\n",
      "Ep 29 | Val Loss: 0.6057 | F1 Macro: 0.7823 (Humor F1: 0.7458)\n",
      "Ep 30 | Val Loss: 0.6319 | F1 Macro: 0.8062 (Humor F1: 0.7561)\n",
      "Ep 31 | Val Loss: 0.6289 | F1 Macro: 0.8029 (Humor F1: 0.7545)\n",
      "Ep 32 | Val Loss: 0.6194 | F1 Macro: 0.8052 (Humor F1: 0.7566)\n",
      "Ep 33 | Val Loss: 0.6187 | F1 Macro: 0.7993 (Humor F1: 0.7525)\n",
      "Ep 34 | Val Loss: 0.6180 | F1 Macro: 0.8026 (Humor F1: 0.7592)\n",
      "Ep 35 | Val Loss: 0.6184 | F1 Macro: 0.7947 (Humor F1: 0.7530)\n",
      "Ep 36 | Val Loss: 0.6331 | F1 Macro: 0.8035 (Humor F1: 0.7551)\n",
      "Ep 37 | Val Loss: 0.5956 | F1 Macro: 0.8001 (Humor F1: 0.7581)\n",
      "Ep 38 | Val Loss: 0.6337 | F1 Macro: 0.8041 (Humor F1: 0.7557)\n",
      "Ep 39 | Val Loss: 0.6229 | F1 Macro: 0.7988 (Humor F1: 0.7551)\n",
      "Ep 40 | Val Loss: 0.6139 | F1 Macro: 0.8074 (Humor F1: 0.7652)\n",
      "Ep 41 | Val Loss: 0.6140 | F1 Macro: 0.8015 (Humor F1: 0.7584)\n",
      "Early Stopping activado.\n",
      "\n",
      "Entrenamiento finalizado. Mejor F1 Macro: 0.8124\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preparar Tensores\n",
    "X_train_t = torch.from_numpy(X_tr).float()\n",
    "Y_train_t = torch.from_numpy(Y_train).long()\n",
    "X_val_t = torch.from_numpy(X_val).float()\n",
    "Y_val_tensor = torch.from_numpy(Y_val).long().to(device)\n",
    "\n",
    "# Parámetros de la red\n",
    "input_size = X_tr.shape[1]\n",
    "output_size = 2   # 2 clases\n",
    "\n",
    "# Inicializar Modelo\n",
    "model = MLP(input_size=input_size, output_size=output_size)\n",
    "model.to(device)\n",
    "\n",
    "classes = np.unique(Y_train)\n",
    "weights_calc = compute_class_weight('balanced', classes=classes, y=Y_train)\n",
    "\n",
    "\n",
    "weights_calc[1] = weights_calc[1] * FACTOR_BALANCE\n",
    "weights = torch.tensor(weights_calc).float().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4)\n",
    "\n",
    "# Variables Early Stopping\n",
    "best_f1 = 0.0\n",
    "patience = 20 \n",
    "epochs_no_improve = 0\n",
    "best_model_path = 'mejor_modelo_fusion.pth'\n",
    "\n",
    "print(\"--- Iniciando Entrenamiento Fusión (900 inputs) ---\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    dataloader = create_minibatches(X_train_t, Y_train_t, batch_size=BATCH_SIZE)\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X_b, y_b in dataloader:\n",
    "        X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_b)\n",
    "        loss = criterion(output, y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # --- VALIDACIÓN ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_v = X_val_t.to(device)\n",
    "        logits = model(X_v)\n",
    "        val_loss = criterion(logits, Y_val_tensor).item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Métrica Objetivo\n",
    "        f1_macro = f1_score(Y_val, preds, average='macro')\n",
    "        acc = accuracy_score(Y_val, preds)\n",
    "        \n",
    "        # Métricas por clase para ver si detectamos el humor\n",
    "        f1_humor = f1_score(Y_val, preds, pos_label=1, average='binary')\n",
    "\n",
    "    print(f\"Ep {epoch+1} | Val Loss: {val_loss:.4f} | F1 Macro: {f1_macro:.4f} (Humor F1: {f1_humor:.4f})\")\n",
    "    \n",
    "    # Paso del Scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early Stopping basado en MAXIMIZAR F1 MACRO\n",
    "    if f1_macro > best_f1:\n",
    "        best_f1 = f1_macro\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"  ★ ¡Nuevo Récord! Modelo guardado.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early Stopping activado.\")\n",
    "            break\n",
    "\n",
    "# Cargar el mejor y probar\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print(f\"\\nEntrenamiento finalizado. Mejor F1 Macro: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a2e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "X_val_t = torch.from_numpy(X_val).to(torch.float32)\n",
    "\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "X_val_t = X_val_t.to(device)\n",
    "\n",
    "# 2. INFERENCIA\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad(): # Ahorra memoria y cálculo\n",
    "    # Predicción (Logits)\n",
    "    y_pred_logits = model(X_val_t)\n",
    "    \n",
    "    # Obtener la clase con mayor probabilidad (Argmax)\n",
    "    y_pred_class_tensor = torch.argmax(y_pred_logits, dim=1)\n",
    "\n",
    "# 3. POST-PROCESAMIENTO\n",
    "\n",
    "y_pred_val_numpy = y_pred_class_tensor.cpu().numpy()\n",
    "\n",
    "print(y_pred_val_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Matriz de Confusión ---\n",
      "[[857 131]\n",
      " [140 432]]\n",
      "\n",
      "--- Reporte de Clasificación ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Humor     0.8596    0.8674    0.8635       988\n",
      "       Humor     0.7673    0.7552    0.7612       572\n",
      "\n",
      "    accuracy                         0.8263      1560\n",
      "   macro avg     0.8134    0.8113    0.8124      1560\n",
      "weighted avg     0.8257    0.8263    0.8260      1560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"--- Matriz de Confusión ---\")\n",
    "cm = confusion_matrix(Y_val, y_pred_val_numpy)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n--- Reporte de Clasificación ---\")\n",
    "\n",
    "print(classification_report(Y_val, y_pred_val_numpy, digits=4, zero_division='warn', target_names=['No Humor', 'Humor']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b14766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de prueba\n",
      "klass\n",
      "-1    5600\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_test.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de prueba\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X_test = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y_test = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f41314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset de prueba...\n",
      "Procesando vectores...\n",
      "Dimensiones de Test listas: (5600, 900)\n",
      "Predicciones generadas:\n",
      "[0 1 0 ... 1 0 1]\n",
      "\n",
      "Conteo de clases predichas:\n",
      "{np.int64(0): np.int64(3466), np.int64(1): np.int64(2134)}\n"
     ]
    }
   ],
   "source": [
    "# 1. PREPARAR DATOS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 0. CARGAR EL ARCHIVO DE TEST ---\n",
    "\n",
    "print(\"Cargando dataset de prueba...\")\n",
    "dataset_test = pd.read_json(\"../Datasets/dataset_humor_test_embeddings.json\", lines=True)\n",
    "\n",
    "# --- 1. EXTRACCIÓN Y FUSIÓN DE EMBEDDINGS (CRÍTICO) ---\n",
    "\n",
    "print(\"Procesando vectores...\")\n",
    "we_ft_test = np.stack(dataset_test['we_ft'].values) # FastText General\n",
    "we_mx_test = np.stack(dataset_test['we_mx'].values) # FastText México\n",
    "we_es_test = np.stack(dataset_test['we_es'].values) # FastText España\n",
    "\n",
    "\n",
    "X_t = np.concatenate([we_ft_test, we_mx_test, we_es_test], axis=1)\n",
    "\n",
    "print(f\"Dimensiones de Test listas: {X_t.shape}\") \n",
    "\n",
    "\n",
    "# Convertir a tensor float\n",
    "X_testing = torch.from_numpy(X_t).to(torch.float32)\n",
    "\n",
    "\n",
    "device = next(model.parameters()).device \n",
    "X_testing = X_testing.to(device)\n",
    "\n",
    "# 2. INFERENCIA\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad(): \n",
    "    # Predicción de logits\n",
    "    y_pred_test_logits = model(X_testing)\n",
    "\n",
    "# 3. PROCESAR RESULTADOS\n",
    "# Obtener la clase (0 o 1) usando argmax\n",
    "y_pred_test_indices = torch.argmax(y_pred_test_logits, dim=1)\n",
    "\n",
    "\n",
    "y_pred_final = y_pred_test_indices.cpu().numpy()\n",
    "\n",
    "print(\"Predicciones generadas:\")\n",
    "print(y_pred_final)\n",
    "\n",
    "\n",
    "unique, counts = np.unique(y_pred_final, return_counts=True)\n",
    "print(\"\\nConteo de clases predichas:\")\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92828deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_resultados(datos, archivo):\n",
    "\n",
    "    df = pd.DataFrame(datos, columns=['klass'])\n",
    "\n",
    "    df['id'] = df.index + 1\n",
    "\n",
    "    df = df[['id', 'klass']]\n",
    "\n",
    "    df.to_csv(archivo, index=False)\n",
    "\n",
    "    print(f\" {datos} guardado exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11564748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0 1 0 ... 1 0 1] guardado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "guardar_resultados(y_pred_final, f\" neuronas{1024}, {256}, {64}, {LEARNING_RATE}, 900, Embeddings, {BATCH_SIZE}, balance {FACTOR_BALANCE}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
