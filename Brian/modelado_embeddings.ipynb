{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f96208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset...\n",
      "Procesando embeddings...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Cargar el dataset\n",
    "print(\"Cargando dataset...\")\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_train_embeddings.json\", lines=True)\n",
    "\n",
    "# 2. Extraer y concatenar los embeddings\n",
    "print(\"Procesando embeddings...\")\n",
    "we_ft = np.stack(dataset['we_ft'].values) # (N, 300)\n",
    "we_mx = np.stack(dataset['we_mx'].values) # (N, 300)\n",
    "we_es = np.stack(dataset['we_es'].values) # (N, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4976abce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión final de entrada: (10400, 900)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_full = np.concatenate([we_ft, we_mx, we_es], axis=1) \n",
    "\n",
    "\n",
    "Y_full = dataset['klass'].to_numpy()\n",
    "\n",
    "print(f\"Dimensión final de entrada: {X_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b8b0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (9360, 900), Val: (1040, 900)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Dividir en Train y Validation\n",
    "# Usamos stratify para mantener la proporción de humor/no humor\n",
    "X_tr, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_full, Y_full, \n",
    "    test_size=0.10, \n",
    "    random_state=42, \n",
    "    stratify=Y_full\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_tr.shape}, Val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a15c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size,c1,c2,c3):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        input_size_h1 = c1\n",
    "        input_size_h2 = c2\n",
    "        input_size_h3 = c3\n",
    "\n",
    "        # Capa 1: Recibe muchas características\n",
    "        self.fc1 = nn.Linear(input_size, input_size_h1)\n",
    "        self.bn1 = nn.BatchNorm1d(input_size_h1)\n",
    "        self.act1 = nn.LeakyReLU(0.01)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Capa 2: Compresión\n",
    "        self.fc2 = nn.Linear(input_size_h1, input_size_h2)\n",
    "        self.bn2 = nn.BatchNorm1d(input_size_h2)\n",
    "        self.act2 = nn.LeakyReLU(0.01)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Capa 3: Refinamiento\n",
    "        self.fc3 = nn.Linear(input_size_h2, input_size_h3)\n",
    "        self.bn3 = nn.BatchNorm1d(input_size_h3)\n",
    "        self.act3 = nn.LeakyReLU(0.01)\n",
    "        self.drop3 = nn.Dropout(0.3)\n",
    "\n",
    "        # Salida\n",
    "        self.output = nn.Linear(input_size_h3, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.act1(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(self.act2(self.bn2(self.fc2(x))))\n",
    "        x = self.drop3(self.act3(self.bn3(self.fc3(x))))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d59360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    # Recibe los documentos en X y las etiquetas en Y\n",
    "    dataset = TensorDataset(X, Y) # Cargar los datos en un dataset de tensores\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3e4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 5e-4#0.00025\n",
    "EPOCHS = 100 \n",
    "FACTOR_BALANCE = 2.75\n",
    "CAPA_1 = 4096\n",
    "CAPA_2 = 2048\n",
    "CAPA_3 = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17e2b41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Entrenamiento Fusión (900 inputs) ---\n",
      "Ep 1 | Val Loss: 0.4048 | F1 Macro: 0.7478 (Humor F1: 0.7240)\n",
      "  ★ ¡Nuevo Récord! Modelo guardado.\n",
      "Ep 2 | Val Loss: 0.4429 | F1 Macro: 0.7767 (Humor F1: 0.7346)\n",
      "  ★ ¡Nuevo Récord! Modelo guardado.\n",
      "Ep 3 | Val Loss: 0.5089 | F1 Macro: 0.6368 (Humor F1: 0.6532)\n",
      "Ep 4 | Val Loss: 0.4533 | F1 Macro: 0.7453 (Humor F1: 0.7189)\n",
      "Ep 5 | Val Loss: 0.5975 | F1 Macro: 0.7726 (Humor F1: 0.7322)\n",
      "Ep 6 | Val Loss: 0.7900 | F1 Macro: 0.7839 (Humor F1: 0.7287)\n",
      "  ★ ¡Nuevo Récord! Modelo guardado.\n",
      "Ep 7 | Val Loss: 0.6489 | F1 Macro: 0.7530 (Humor F1: 0.7157)\n",
      "Ep 8 | Val Loss: 0.6379 | F1 Macro: 0.7136 (Humor F1: 0.6979)\n",
      "Ep 9 | Val Loss: 0.7746 | F1 Macro: 0.7144 (Humor F1: 0.6911)\n",
      "Ep 10 | Val Loss: 0.7368 | F1 Macro: 0.7619 (Humor F1: 0.7203)\n",
      "Ep 11 | Val Loss: 0.8186 | F1 Macro: 0.7548 (Humor F1: 0.7143)\n",
      "Ep 12 | Val Loss: 0.9547 | F1 Macro: 0.7837 (Humor F1: 0.7298)\n",
      "Ep 13 | Val Loss: 0.9034 | F1 Macro: 0.7654 (Humor F1: 0.7181)\n",
      "Ep 14 | Val Loss: 0.9291 | F1 Macro: 0.7662 (Humor F1: 0.7232)\n",
      "Ep 15 | Val Loss: 1.2256 | F1 Macro: 0.7842 (Humor F1: 0.7224)\n",
      "  ★ ¡Nuevo Récord! Modelo guardado.\n",
      "Ep 16 | Val Loss: 0.8693 | F1 Macro: 0.7467 (Humor F1: 0.7138)\n",
      "Ep 17 | Val Loss: 1.0465 | F1 Macro: 0.7660 (Humor F1: 0.7176)\n",
      "Ep 18 | Val Loss: 1.2431 | F1 Macro: 0.7853 (Humor F1: 0.7292)\n",
      "  ★ ¡Nuevo Récord! Modelo guardado.\n",
      "Ep 19 | Val Loss: 1.2583 | F1 Macro: 0.7753 (Humor F1: 0.7189)\n",
      "Ep 20 | Val Loss: 1.0535 | F1 Macro: 0.7674 (Humor F1: 0.7254)\n",
      "Ep 21 | Val Loss: 1.2411 | F1 Macro: 0.7773 (Humor F1: 0.7234)\n",
      "Ep 22 | Val Loss: 1.1555 | F1 Macro: 0.7796 (Humor F1: 0.7309)\n",
      "Ep 23 | Val Loss: 1.1681 | F1 Macro: 0.7745 (Humor F1: 0.7268)\n",
      "Ep 24 | Val Loss: 1.2213 | F1 Macro: 0.7749 (Humor F1: 0.7257)\n",
      "Ep 25 | Val Loss: 1.1905 | F1 Macro: 0.7698 (Humor F1: 0.7217)\n",
      "Ep 26 | Val Loss: 1.3516 | F1 Macro: 0.7817 (Humor F1: 0.7273)\n",
      "Ep 27 | Val Loss: 1.1239 | F1 Macro: 0.7680 (Humor F1: 0.7225)\n",
      "Ep 28 | Val Loss: 1.2986 | F1 Macro: 0.7748 (Humor F1: 0.7213)\n",
      "Ep 29 | Val Loss: 1.2733 | F1 Macro: 0.7750 (Humor F1: 0.7220)\n",
      "Ep 30 | Val Loss: 1.2211 | F1 Macro: 0.7765 (Humor F1: 0.7268)\n",
      "Ep 31 | Val Loss: 1.2805 | F1 Macro: 0.7795 (Humor F1: 0.7284)\n",
      "Ep 32 | Val Loss: 1.3161 | F1 Macro: 0.7829 (Humor F1: 0.7314)\n",
      "Ep 33 | Val Loss: 1.2572 | F1 Macro: 0.7783 (Humor F1: 0.7286)\n",
      "Ep 34 | Val Loss: 1.3491 | F1 Macro: 0.7741 (Humor F1: 0.7192)\n",
      "Ep 35 | Val Loss: 1.2835 | F1 Macro: 0.7770 (Humor F1: 0.7264)\n",
      "Ep 36 | Val Loss: 1.2818 | F1 Macro: 0.7735 (Humor F1: 0.7235)\n",
      "Ep 37 | Val Loss: 1.3715 | F1 Macro: 0.7810 (Humor F1: 0.7270)\n",
      "Ep 38 | Val Loss: 1.3125 | F1 Macro: 0.7807 (Humor F1: 0.7282)\n",
      "Early Stopping activado.\n",
      "\n",
      "Entrenamiento finalizado. Mejor F1 Macro: 0.7853\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preparar Tensores\n",
    "X_train_t = torch.from_numpy(X_tr).float()\n",
    "Y_train_t = torch.from_numpy(Y_train).long()\n",
    "X_val_t = torch.from_numpy(X_val).float()\n",
    "Y_val_tensor = torch.from_numpy(Y_val).long().to(device)\n",
    "\n",
    "# Parámetros de la red\n",
    "input_size = X_tr.shape[1]\n",
    "output_size = 2   # 2 clases\n",
    "\n",
    "# Inicializar Modelo\n",
    "model = MLP(input_size=input_size, output_size=output_size, c1=CAPA_1, c2=CAPA_2, c3=CAPA_3)\n",
    "model.to(device)\n",
    "\n",
    "classes = np.unique(Y_train)\n",
    "weights_calc = compute_class_weight('balanced', classes=classes, y=Y_train)\n",
    "\n",
    "\n",
    "weights_calc[1] = weights_calc[1] * FACTOR_BALANCE\n",
    "weights = torch.tensor(weights_calc).float().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.005)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4)\n",
    "\n",
    "# Variables Early Stopping\n",
    "best_f1 = 0.0\n",
    "patience = 20 \n",
    "epochs_no_improve = 0\n",
    "best_model_path = 'mejor_modelo_fusion.pth'\n",
    "\n",
    "print(\"--- Iniciando Entrenamiento Fusión (900 inputs) ---\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    dataloader = create_minibatches(X_train_t, Y_train_t, batch_size=BATCH_SIZE)\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X_b, y_b in dataloader:\n",
    "        X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_b)\n",
    "        loss = criterion(output, y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # --- VALIDACIÓN ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_v = X_val_t.to(device)\n",
    "        logits = model(X_v)\n",
    "        val_loss = criterion(logits, Y_val_tensor).item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Métrica Objetivo\n",
    "        f1_macro = f1_score(Y_val, preds, average='macro')\n",
    "        acc = accuracy_score(Y_val, preds)\n",
    "        \n",
    "        # Métricas por clase para ver si detectamos el humor\n",
    "        f1_humor = f1_score(Y_val, preds, pos_label=1, average='binary')\n",
    "\n",
    "    print(f\"Ep {epoch+1} | Val Loss: {val_loss:.4f} | F1 Macro: {f1_macro:.4f} (Humor F1: {f1_humor:.4f})\")\n",
    "    \n",
    "    # Paso del Scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early Stopping basado en MAXIMIZAR F1 MACRO\n",
    "    if f1_macro > best_f1:\n",
    "        best_f1 = f1_macro\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"  ★ ¡Nuevo Récord! Modelo guardado.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early Stopping activado.\")\n",
    "            break\n",
    "\n",
    "# Cargar el mejor y probar\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print(f\"\\nEntrenamiento finalizado. Mejor F1 Macro: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a2e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "X_val_t = torch.from_numpy(X_val).to(torch.float32)\n",
    "\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "X_val_t = X_val_t.to(device)\n",
    "\n",
    "# 2. INFERENCIA\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad(): # Ahorra memoria y cálculo\n",
    "    # Predicción (Logits)\n",
    "    y_pred_logits = model(X_val_t)\n",
    "    \n",
    "    # Obtener la clase con mayor probabilidad (Argmax)\n",
    "    y_pred_class_tensor = torch.argmax(y_pred_logits, dim=1)\n",
    "\n",
    "# 3. POST-PROCESAMIENTO\n",
    "\n",
    "y_pred_val_numpy = y_pred_class_tensor.cpu().numpy()\n",
    "\n",
    "print(y_pred_val_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dad36d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor umbral es: 0.5999999999999999 con F1: 0.8127772599533257\n"
     ]
    }
   ],
   "source": [
    "# 1. Obtén las probabilidades en lugar de las clases directas\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_val_t)\n",
    "    probs = torch.softmax(outputs, dim=1)[:, 1] # Probabilidad de clase 1 (Humor)\n",
    "\n",
    "# 2. Barre umbrales para encontrar el mejor F1\n",
    "mejores_preds = []\n",
    "mejor_f1_t = 0\n",
    "mejor_umbral = 0.5\n",
    "\n",
    "for umbral in np.arange(0.3, 0.7, 0.05):\n",
    "    preds_umbral = (probs.cpu().numpy() > umbral).astype(int)\n",
    "    f1_t = f1_score(Y_val, preds_umbral, average='macro')\n",
    "    \n",
    "    if f1_t > mejor_f1_t:\n",
    "        mejor_f1_t = f1_t\n",
    "        mejor_umbral = umbral\n",
    "\n",
    "print(f\"El mejor umbral es: {mejor_umbral} con F1: {mejor_f1_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3bf7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Matriz de Confusión ---\n",
      "[[824 164]\n",
      " [116 456]]\n",
      "\n",
      "--- Reporte de Clasificación ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Humor     0.8766    0.8340    0.8548       988\n",
      "       Humor     0.7355    0.7972    0.7651       572\n",
      "\n",
      "    accuracy                         0.8205      1560\n",
      "   macro avg     0.8060    0.8156    0.8099      1560\n",
      "weighted avg     0.8249    0.8205    0.8219      1560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"--- Matriz de Confusión ---\")\n",
    "cm = confusion_matrix(Y_val, y_pred_val_numpy)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n--- Reporte de Clasificación ---\")\n",
    "\n",
    "print(classification_report(Y_val, y_pred_val_numpy, digits=4, zero_division='warn', target_names=['No Humor', 'Humor']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b14766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de prueba\n",
      "klass\n",
      "-1    5600\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"../Datasets/dataset_humor_test.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de prueba\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X_test = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y_test = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61f41314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset de prueba...\n",
      "Procesando vectores...\n",
      "Dimensiones de Test listas: (5600, 900)\n",
      "Predicciones generadas:\n",
      "[0 0 0 ... 1 0 1]\n",
      "\n",
      "Conteo de clases predichas:\n",
      "{np.int64(0): np.int64(3428), np.int64(1): np.int64(2172)}\n"
     ]
    }
   ],
   "source": [
    "# 1. PREPARAR DATOS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 0. CARGAR EL ARCHIVO DE TEST ---\n",
    "\n",
    "print(\"Cargando dataset de prueba...\")\n",
    "dataset_test = pd.read_json(\"../Datasets/dataset_humor_test_embeddings.json\", lines=True)\n",
    "\n",
    "# --- 1. EXTRACCIÓN Y FUSIÓN DE EMBEDDINGS (CRÍTICO) ---\n",
    "\n",
    "print(\"Procesando vectores...\")\n",
    "we_ft_test = np.stack(dataset_test['we_ft'].values) # FastText General\n",
    "we_mx_test = np.stack(dataset_test['we_mx'].values) # FastText México\n",
    "we_es_test = np.stack(dataset_test['we_es'].values) # FastText España\n",
    "\n",
    "\n",
    "X_t = np.concatenate([we_ft_test, we_mx_test, we_es_test], axis=1)\n",
    "\n",
    "print(f\"Dimensiones de Test listas: {X_t.shape}\") \n",
    "\n",
    "\n",
    "# Convertir a tensor float\n",
    "X_testing = torch.from_numpy(X_t).to(torch.float32)\n",
    "\n",
    "\n",
    "device = next(model.parameters()).device \n",
    "X_testing = X_testing.to(device)\n",
    "\n",
    "# 2. INFERENCIA\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad(): \n",
    "    # Predicción de logits\n",
    "    y_pred_test_logits = model(X_testing)\n",
    "\n",
    "# 3. PROCESAR RESULTADOS\n",
    "# Obtener la clase (0 o 1) usando argmax\n",
    "y_pred_test_indices = torch.argmax(y_pred_test_logits, dim=1)\n",
    "\n",
    "\n",
    "y_pred_final = y_pred_test_indices.cpu().numpy()\n",
    "\n",
    "print(\"Predicciones generadas:\")\n",
    "print(y_pred_final)\n",
    "\n",
    "\n",
    "unique, counts = np.unique(y_pred_final, return_counts=True)\n",
    "print(\"\\nConteo de clases predichas:\")\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92828deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_resultados(datos, archivo):\n",
    "\n",
    "    df = pd.DataFrame(datos, columns=['klass'])\n",
    "\n",
    "    df['id'] = df.index + 1\n",
    "\n",
    "    df = df[['id', 'klass']]\n",
    "\n",
    "    df.to_csv(archivo, index=False)\n",
    "\n",
    "    print(f\" {datos} guardado exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11564748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0 0 0 ... 1 0 1] guardado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "guardar_resultados(y_pred_final, f\" neuronas{CAPA_1}, {CAPA_2}, {CAPA_3}, {LEARNING_RATE}, 900, Embeddings, {BATCH_SIZE}, balance {FACTOR_BALANCE}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
